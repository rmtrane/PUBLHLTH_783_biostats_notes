---
title: "Biostats Lecture 5:<br>Statistical Hypothesis Testing"
subtitle: "Public Health 783"
author: "Ralph Trane"
institute: "University of Wisconsin--Madison<br><br>"
date: "Fall 2019"
output:
  xaringan::moon_reader:
    self_contained: true
    css: [../css/uwmadison.css, default-fonts, ../css/extra-classes.css]
    lib_dir: libs
    nature:
      titleSlideClass: [center, top, .title-slide]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      #ratio: '16:9'
      navigation:
        scroll: false
---

# Recap

Random variables

Distributions

Estimators

Estimators are random variables! (for example, the average is a random variable)

---
layout: true

# Statistical Hypothesis Testing

---

```{r include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)
library(SASmarkdown)

theme_set(theme_bw())

library(viridis)
scale_color_continuous <- scale_colour_continuous <- function(...) scale_color_viridis_c(...)
scale_color_discrete <- scale_colour_discrete <- function(...) scale_color_viridis_d(...)

scale_fill_continuous <- function(...) scale_fill_viridis_c(...)
scale_fill_discrete <- function(...) scale_fill_viridis_d(...)

n_rolls <- 27
XX <- 10
```


**Scenario**:

We've been playing a simple game. Everytime you roll a six, I pay you a dollar. Everytime I roll a six, you pay me a dollar. 

I've had crazy good luck, and by the end of the day won a lot of money from you.

--

You accuse me of cheating, and demand to test the dice I've been using!

I agree to let you test them, but ONLY if you do it in a sound, statistical manner. How to go about that?

--

You decide to roll the dice $`r n_rolls/9`$ times each, for a total of $`r n_rolls`$ rolls. You assume they'll all behave the same, so the probability of rolling a six is the same for all three dice. 

---

**Setup**:

Let $X_1, X_2, ..., X_{`r n_rolls`}$ be the outcomes of the thirty "trials". Each trial consists of rolling a die, and check if it's a six or not. If it's a six, we'll call it a success, if not we'll call it a failure. I.e. $X_i \sim$
--
 $\text{Bernoulli}(\pi)$. 

*IF* the dice are fair, $P(X_i = 1) = 1/6$ for all $i = 1,2,...,`r n_rolls`$. 

*IF* the dice are fair, we would expect to roll a $6$ close to $\frac{1}{6}\cdot `r n_rolls` = 6$ times, i.e. about $5$ of the $X$'s should be $1$'s. 

What would cause you to reject the idea that the dice are fair? 

--

If we see way more than $6$ sixes. What would be "way more"? $7$? $8$? $17$?

---

In terms of probabilities: what is the *probability* of observing at least $`r XX`$ sixes *IF* the $P(X_i = 1) = \frac{1}{6}$? 

* if the probability is small, $`r XX`$ is a lot of sixes
* if the probability is large, $`r XX`$ is a reasonable number of sixes

First, introduce the random variable $Y =$ number of sixes $= X_1 + X_2 + ... X_{`r n_rolls`}$. The probability of observing more than $`r XX`$ sixes is $P(Y \ge `r XX`)$. To find this, we need the distribution of $Y$, which is 
--
 $\text{Binomial}(`r n_rolls`, \pi)$, where $\pi$ is the probability of rolling a six. 

---

*IF* the dice are fair, $\pi = \frac{1}{6}$. So *IF* the dice are fair, the distribution of $Y$ looks like this:


<center>
```{r out.width = 600, out.height = 450}
binom_dist <- ggplot(data = tibble(x = 0:n_rolls) %>% 
                       mutate(y = dbinom(x, size = n_rolls, prob = 1/6),
                              Probability = round(y, digits = 5)),
       aes(x = x, y = y, label = Probability)) + 
  geom_bar(stat = 'identity', 
           width = 1) +
  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +
  labs(title = paste0('Binomial(n = ', n_rolls, ', \u03C0 = 1/6)'),
       x = 'Value of Y', y = 'Probability')

plotly::ggplotly(binom_dist, tooltip = c("label", "x"))
```
</center>

---

The probability we want to find is the red area below. We will do this in SAS in just a second. The result is `r (p_val <- round(1-pbinom(XX, size = n_rolls, prob = 1/6), digits = 5))`.

<center>
```{r out.width = 600, out.height = 450}
binom_dist_2 <- binom_dist + 
  geom_vline(xintercept = XX-0.5, linetype = 'dashed') +
  aes(fill = if_else(x < XX, paste("Y <", XX), paste("Y \u2265", XX))) +
  scale_fill_manual('', values = c("grey35", "red")) +
  theme(legend.position = 'none')

plotly::ggplotly(binom_dist_2, tooltip = c("label", "x"))
```
</center>

---

This means that *IF* the true probability of rolling a six with these dice is indeed $\frac{1}{6}$, the probability of rolling $`r XX`$ or more sixes is $`r p_val`$. This probability is called the *p-value* for the test $H_0: \pi = 1/6$ when testing against $H_A: \pi > 1/6$. 

Is this small enough to convince you that the true probability is *NOT* $\frac{1}{6}$?

---

</br>
</br>
</br>
</br>

.center[
  **In SAS.**
]


---

## Different (but the same) approach

```{r}
p_obs <- XX/n_rolls
```


Instead of looking at $Y$ (number of sixes), look at the proportion of sixes. I.e. 

$$\hat{p} = \frac{Y}{n} = \frac{1}{`r n_rolls`}(X_1 + ... + X_{`r n_rolls`}) = \frac{1}{`r n_rolls`} \sum_{i=1}^{`r n_rolls`} X_i$$


This is an average, so CLT tells us it's normally distributed around the true value of $\pi$, and variance $\frac{\text{Var}(X_i)}{n} = \frac{\pi(1-\pi)}{`r n_rolls`}$. 

We reject the idea that the true value of $\pi$ is $\frac{1}{6}$ when $\hat{p}$ is "far from" $\frac{1}{6}$. 

I.e. when $\hat{p} - \frac{1}{6}$ is large. When this is large, so is $Z = \frac{\hat{p}-1/6}{\text{SD}(\hat{p})}$. 

If we pretend $\pi = \frac{1}{6}$, then $Z \sim N(0,1)$!

If we pretend $\pi = \frac{1}{6}$, then $\text{SD}(\hat{p}) = \sqrt{\text{Var}(\hat{p})} = \sqrt{\frac{1/6 \cdot (1-1/6)}{`r n_rolls`}}$, and so we can actually calculate $z_{obs}$, the observed value of $Z$.

---

## Different (but the same) approach

So, 

\begin{align*}
\text{p-value} &= P(\hat{p} > p_{obs}) \\
               &= P\left(\frac{\hat{p} - 1/6}{\text{SD}(\hat{p})} > \frac{p_{obs} - 1/6}{\sqrt{1/6 \cdot (1-1/6)/`r n_rolls`}} \right)
               &= P(Z > `r (z_obs <- round((p_obs - 1/6)/(sqrt(1/6*5/6)/sqrt(n_rolls)), digits = 5))`)
\end{align*}

---


So we found our p-value as the area depicted below. Turns out, this is `r round(1-pnorm(z_obs), digits = 5)`. 

```{r fig.width = 4, fig.height = 3, out.width = 450, out.height = 300, fig.align='center', dpi = 300}
ggplot(data = tibble(x = seq(-4, 4, by = 0.01)),
       aes(x = x)) + 
  stat_function(fun = dnorm) + 
  geom_area(data = tibble(x = seq(z_obs, 4, by = 0.01)),
            aes(x = x, y = dnorm(x))) +
  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1)))
```

This strategy can be used every time we the quantity of interest in follows a (at least approximately) normal distribution. Subtract the mean and divide by the standard deviation to get the standard normal. Then find the probability. 


---

## Group Comparison

Two groups, 15 observations in each group. Want to test $H_0: \mu_X = \mu_Y$ against $H_A: \mu_X \neq \mu_Y$. 

```{r fig.width = 4, fig.height = 3, out.width = 450, out.height = 300, fig.align='center', dpi = 300}
X <- rnorm(mean = 2, sd = 5, n = 15)
Y <- rnorm(mean = 5, sd = 3, n = 15)
group_comparison <- tibble(group = rep(c("X", "Y"), each = 15),
                           obs = c(X, Y))
ggplot(group_comparison,
       aes(x = group, y = obs)) + 
  geom_boxplot(coef = 10) + 
  geom_jitter(width = 0.1, height = 0)
```


---

Natural to look at $\bar{X} - \bar{Y}$.

If $n$ is "large enough", both of these quantities are normally distributed. From last week, normal minus normal is normal. So $\bar{X} - \bar{Y}$ is normal!

Next question: exactly what normal would it be *IF* we pretend the null hypothesis is true? 

Mean: $E(\bar{X} - \bar{Y}) = E(\bar{X}) - E(\bar{Y})=$
--
 $\mu_X - \mu_Y =$
--
 $0$

Variance (assuming independence): $\text{Var}(\bar{X} - \bar{Y}) =$
--
 $\text{Var}(\bar{X}) + \text{Var}(\bar{Y}) = \frac{\sigma_X^2}{n_X} + \frac{\sigma_Y^2}{n_Y}$.
 
So $Z = \frac{\bar{X} - \bar{Y}}{\text{SD}(\bar{X} - \bar{Y})} = \frac{\bar{X} - \bar{Y}}{\sqrt{\text{Var}(\bar{X} - \bar{Y})}} \sim N(0,1)$. 

What's the problem here?
--
 We don't know $\sigma_X^2$ or $\sigma_Y^2$!

---

Luckily, we can simply *estimate* both, and plug them in. 

When we do this, the result no longer follows a standard normal distribution...
--
 BUT it follows a particular $t$-distribution. How to calculate the exact degrees of freedom is a bit tricky, but it can be done.
 
**For the patient type:**

The degrees of freedom is 

\begin{align*}
  \text{df} &= n_X + n_Y - 2
\end{align*}

if we are willing to assume $\sigma_X^2 = \sigma_Y^2$. In this case we would use $s_X^2 = s_Y^2 = \frac{1}{n_X + n_Y - 2}\left(\sum_{i=1}^{n_X} (x_i - \bar{x})^2 + \sum_{i=1}^{n_Y} (y_i - \bar{y})^2\right)$. 

---

If you're not willing to assume $\sigma_X^2 = \sigma_Y^2$, the degrees of freedom is

\begin{align*}
  \text{df} &= \frac{\left[\frac{s_X^2}{n_X} + \frac{s_Y^2}{n_Y}\right]^2}{\frac{(s_X^2/n_X)^2}{n_X - 1} + \frac{(s_Y^2/n_Y)^2}{n_Y - 1}},
\end{align*}

and we would then use the regular sample variances as estimates:

\begin{align*}
  s_X^2 &= \frac{1}{n_X - 1} \sum_{i=1}^{n_X} (x_i - \bar{x})^2 \\
  &\\
  s_Y^2 &= \frac{1}{n_Y - 1} \sum_{i=1}^{n_Y} (y_i - \bar{y})^2
\end{align*}

---

**For the not so patient type:**

Don't worry about it, you'll use software to calculate this. 

HOWEVER, you will need to know WHEN you can assume equal variance.

