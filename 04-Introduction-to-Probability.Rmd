# (PART) Introduction to Probability {-}

Loosely based on @ls chapter 5.

# What is "probability"? 

A *probability* is a number between 0 and 1 that indicates how likely it is that a certain event happens. An event that has the probability of 1 **always** occurs, while an event with probability of 0 **never** occurs. Every number in between are a bit harder to interpret. 

For example, an event with probability 0.5 supposedly happens every other time. This makes sense if you think about something that can be repeated, such as a coin flip, or the roll of a die, but how does that work if we consider an event that only occurs once? For example, how do we interpret a weather forecast that claims there's a 0.5 (i.e. 50\%) chance of rain tomorrow? We can only observe if it rains tomorrow or not once, so the probability surely must be 0 (it doesn't rain) or 1 (it rains), right?

## Definitions

As hinted at above, the concept of "probability" can be a bit challenging to wrap your head around. There are generally two ways that the term is introduced. Though they are very similar once you understand the concepts, they can seem radically different at first. 

```{definition, prob-def-1, echo = TRUE}
The probability of an event is the number of outcomes that ensure the event happens divided by the total number of possible outcomes, **IF** all outcomes are equally likely:

$$
  P(\text{event}) = \frac{\text{number of outcomes that result in event}}{\text{total number of possible outcomes}}.
$$
```

We often refer to the numerator in this fraction as the number of favorable outcomes. 

I want to take a second to draw your attention to that small, but incredibly important, final bit of the definition: "IF all outcomes are equally likely". We will later discuss what to do if this is not the case, but for now, this will be an underlying assumption. 

The best way to become comfortable with this definition is by considering a few simple examples. The following two examples are the most commonly used, and by far most boring, examples in the history of statistics. However, they are super useful for two reasons: 

1. They are so simple that it is possible to better grasp what's going on
2. A lot of more complicated examples can be simplified by comparing them to these two

### Example: coin flip {-}

We want to find the probability $P(\text{coin comes up heads})$. A natural assumption is that when flipping a coin, heads and tails are the only outcomes^[i.e. it is NOT possible for the coin to land on the side], and they are equally likely. Therefore, 

\begin{align}
  P(\text{coin comes up heads}) &= \frac{\text{# possible outcomes that come up heads}}{\text{# possible outcomes}} \\
  &= \frac{1}{2} \\
  &= 0.5.
\end{align}

Similarly, one can find the probability that the coin comes up tails:

\begin{align}
  P(\text{coin comes up tails}) &= \frac{\text{# possible outcomes that come up tails}}{\text{# possible outcomes}} \\
  &= \frac{1}{2} \\
  &= 0.5.
\end{align}

### Example: roll of a die {-}

--- 

The two examples above show situations where all possible outcomes are equally likely. What if that is not the case? 

### Example: disease status {-}

Let us consider the SHOW data set. We might be interested in the probability of a subject being obese. Now, there seems to be only two outcomes here: either the subject is obese, or the subject is not. So, using the same string of thoughts as above, one might  conclude that the probability of a subject being obese if $\frac{1}{2}$, i.e. $0.5$. 

This is obviously not the case. The problem with this approach is that the two outcomes -- those being "the subject is obese", and "the subject is NOT obese" -- are not equally likely, so the simple approach of simply dividing the number of favorable outcomes by the number of possible outcomes is not doing us any good. 

--- 

To find a more satisfying answer to the question asked in the last example, we need to consider a different approach to probabilities.

```{definition, echo = TRUE, label = prob-def-2}
The probability of a specific outcome from an experiment is the proportion of times the outcome occurs if the experiment is repeated an *infinite number of times*.
```

Repeating an experiment an infinite number of times is obviously not possible, so in practice "an infinite number of times" becomes "a very large number of times". 

When introducing this different approach to probabilities, first we need to make sure it doesn't contradict our previous approach. 

### Example: coin flip (revisited) {-}

We previously established that when flipping a coin, the probability of heads is $0.5$. Hopefully this new definition will yield a similar answer. 

To find out if that is actually the case, we would have to flip a coin "an infinite number of times". Obviously, this is not possible, so we will have to settle for "a very large number of times". So, imagine we flip a coin `r n_flips <- 100000; format(n_flips, scientific = FALSE)` times. Every time it is flipped, we write down the result, and count how many times we've seen heads, and how many times we've seen tails so far. If the probability of seeing heads is $0.5$, we should eventually see about as many heads as tails. 

Below is an animation that shows the results of such an experiment. The bars show you the proportion of heads and tails, which in the end (by the definition above) will converge to the probability. The first 100 flips are all shown, then only the results after every 100 flips, and finally results after every 1000 flips are shown. Note how at the very end the two bars are both very close to $0.5$.

```{r}
set.seed(1010101)
coin_flip <- tibble(outcome = rbinom(n = n_flips, size = 1, prob = 0.5)) %>% 
  mutate(n = row_number(),
         HT = if_else(outcome == 1, 'heads', 'tails'),
         heads = cumsum(outcome),
         tails = n - heads) %>%
  gather(key = 'outcome', value = 'count', heads, tails) %>%
  mutate(props = count/n) %>% 
  filter(n < 101 | (n %% 100 == 0 & n < 10000) | n %% 1000 == 0)
```

```{r eval = FALSE, include = FALSE}
library(gganimate)

coin_flip_anim <- coin_flip %>% 
  ggplot(aes(x = outcome, y = props, 
             label = paste0("Number of ", outcome, ":\n", count))) + 
    geom_bar(stat = 'identity') + 
    scale_y_continuous(expand = expand_scale(0,0)) + 
    geom_text(size = 6, aes(y = 0.1), fontface = "bold", color = 'black') +
    geom_hline(yintercept = 1/2, linetype = 'dashed', color = 'red') + 
    labs(title = "Number of total coin flips: {unique(coin_flip$n)[frame]}") + 
    theme_bw() + 
    theme(title = element_text(size = 20)) +
    transition_manual(frames = n)

anim_save(filename = "coin_flip.mp4", 
          animation = coin_flip_anim, 
          nframes = nrow(coin_flip_anim$data)/2,
          renderer = ffmpeg_renderer(format = ".mp4"))
```

<video controls>
  <source src="coin_flip.mp4" type="video/mp4">
</video>

### Example: roll of a die (revisited)

`r n_rolls <- 100000; format(n_rolls, scientific = FALSE)`

```{r}
die <- tibble(roll = sample(1:6, size = n_rolls, replace = TRUE)) %>% 
  transmute(n = row_number(),
            `1` = cumsum(roll == 1),
            `2` = cumsum(roll == 2),
            `3` = cumsum(roll == 3),
            `4` = cumsum(roll == 4),
            `5` = cumsum(roll == 5),
            `6` = cumsum(roll == 6)) %>% 
  gather(key = 'n eyes', value = 'count', -n) %>% 
  mutate(props = count/n) %>% 
  filter(n < 101 | (n %% 100 == 0 & n < 10000) | n %% 1000 == 0)
```

```{r include = FALSE, eval = FALSE}
die_roll_anim <- die %>% 
  ggplot(aes(x = `n eyes`, y = props, label = paste0("# of ", `n eyes`, "'s:\n", count))) + 
    geom_bar(stat = 'identity') + 
    scale_y_continuous(limits = c(0,1), expand = expand_scale(0,0))  +
    geom_text(size = 4, aes(y = 0.1), fontface = "bold", color = 'black') +
    geom_hline(yintercept = 1/6, linetype = 'dashed', color = 'red') + 
    labs(title = "Number of total rolls: {unique(die$n)[frame]}") + 
    theme_bw() + 
    theme(title = element_text(size = 20)) +
    transition_manual(frames = n)

anim_save(filename = "die_roll.mp4", 
          animation = die_roll_anim, 
          nframes = nrow(die_roll_anim$data)/6,
          renderer = ffmpeg_renderer(format = ".mp4"))
```

<video controls>
  <source src="die_roll.mp4" type="video/mp4">
</video>

### Example: disease status

Okay, so both when flipping a coin and rolling a die, the second definition agrees with the first one. But how can we use this way of thinking in the disease status example? What does it even mean to "repeat the experiment", let alone "repeat an infinite number of times"?!

In such a situation, we make a (very crude, but very necessary) assumption: we assume that all the subjects in the cohort are "similar enough" that we can pretend that observing the disease status of multiple people constitutes multiple experiments. We then estimate the probability of having the disease as the proportion of subjects with the disease. 


# Conditional Probability

So far, we have talked about probabilities in a context where no additional information is available about the experiment. This is of course not always the case, and also not always what we are interested in. 

A useful concept in these cases is the concept of *conditional probabilities*. In a nutshell, conditional probabilities deal with the chances of something happening given something else has already happened. If we consider two events, $A$ and $B$, then we write $P(A | B)$ for the conditional probability of $A$ given that $B$ has happened.

## Example: roll a die {-}

Previously, we considered the probabilities associated with the roll of a die. We found that the probability of rolling a six is $\frac{1}{6}$. What if we somehow knew that the outcome turned out to be an even number, but simply didn't know which even number? Well, using this information, we know there are only three possible outcomes, namely $2,4,6$. They are all equally likely, so using the probability of rolling a six given the roll comes up even is 

$$\left .P(\text{roll a } 6\ \right|\ \text{roll is even}) = \frac{1}{3}.$$

## Example: disease status {-}



## Example: Sensitivity/specificity {-}

Two important examples of conditional probabilities are the so-called sensitivity and specificity. These are particularly useful when discussing the accuracy of screening tests. 

The *sensitivity* of a test is the *true positive rate* (or fraction). That is, out of the tests performed on individuals with the disease of interest, how many come out positive. I.e. $\text{sensitivity} = P(\text{test positive}\ |\ \text{individual diseased})$. 

Similarly, the *specificity* of a test is the *true negative rate* (or fraction), i.e. the proportion of tests performed on healthy individuals that come out negative: $\text{specificity} = P(\text{test negative}\ |\ \text{individual healthy})$. 

It is also often useful to consider the *false positive rate* (FPR) and *false negative rate* (FNR). These are defined as follows:

\begin{align*}
  \text{FPR} &= P(\text{test positive}\ |\ \text{individual healthy}), \\
  \text{FNR} &= P(\text{test negative}\ |\ \text{individual diseased}). \\
\end{align*}

Let's consider a concrete example. Below is table 5-5 from @ls. This table shows the results of screenings of 4810 pregnant women to assess if their fetus is likely to have Down Syndrome. After birth, it is determined if the child actually has Down Syndrome, provided a ground truth that we can check our screening method against. Ideally, the test is positive for all kids with Down Syndrome, and negative for alld kids without Down Syndrome. 

```{r}
prenatal_screening <- tibble(`Fetus Status` = c('Affected', 'Affected', 'Unaffected', 'Unaffected'),
                             `Screening Test Result` = factor(c('Positive', 'Negative', 'Positive', 'Negative'),
                                                              levels = c('Positive', 'Negative')),
                             n = c(9, 1, 351, 4449)) %>% 
  spread(`Fetus Status`, n)

prenatal_screening_DT <- prenatal_screening %>% 
  janitor::as_tabyl() %>% 
  janitor::adorn_totals(where = c('row', 'col')) %>% 
  DT::datatable(options = list(dom = "t", ordering = FALSE), 
                rownames = FALSE,
                width = 10)

prenatal_screening_DT
```

Let us calculate the specificity, sensitivity, FNR, and FPR:

$$
\begin{align*}
  \text{specificity} &= P(\text{test negative}\ |\ \text{child healthy}) \\
                     &= \frac{\text{# negative tests among healthy children}}{\text{# healthy children}} \\
                     &= \frac{4449}{4800} = 0.927 \\
                     & \\
  \text{sensitivity} &= P(\text{test positive}\ |\ \text{child has Down Syndrome}) \\
                     &= \frac{\text{# positive tests among children with Down Syndrome}}{\text{# children with Down Syndrome}} \\
                     &= \frac{9}{10} = 0.9 \\
                     & \\
  \text{FPR} &= P(\text{test positive}\ |\ \text{individual healthy}) \\
             &= \frac{\text{# positive tests among healthy children}}{\text{# healthy children}} \\
             &= \frac{351}{4800} = 0.073 \\
             & \\
  \text{FNR} &= P(\text{test negative}\ |\ \text{individual diseased}) \\
             &= \frac{\text{# negative tests among children with Down Syndrome}}{\text{# children with Down Syndrome}} \\
             &= \frac{1}{10} = 0.1.
\end{align*}
$$

We see that the test has some very desirable attributes, in high specificity AND high sensitivity. At this point, some might stop and wonder for a second: the end goal is to determine if the test is accurate, so why don't we just calculate the accuracy of the test? I.e. what's wrong at simply looking at the number of corret test results out of the total number of tests? Let's take a look.

$$
\begin{align}
  \text{test accuracy} &= \frac{\text{# correct results}}{\text{# tests performed}} \\
                       &= \frac{9 + 4449}{4810} \\
                       &= \frac{4458}{4810} = 0.927
\end{align}
$$

That's pretty impressive. The test has an accuracy rate of almost $93\%$, i.e. almost $93\%$ of tests yield the correct result. Now, let us consider a different test for the same disease. Tested on the same 4810 women, let's pretend it yields the following results:

```{r}
prenatal_screening %>% 
  mutate(`Screening Test Result` = factor(c('Negative', 'Positive'), 
                                          levels = c('Positive', 'Negative'))) %>% 
  arrange(`Screening Test Result`) %>% 
  janitor::as_tabyl() %>% 
  janitor::adorn_totals(where = c('row', 'col')) %>% 
  DT::datatable(options = list(dom = "t", ordering = FALSE), 
                rownames = FALSE,
                width = 10)
```


Now, the accuracy rate of this test is $\frac{1 + 4449}{4810} = 0.925$, i.e. almost the same as the first test. That's, again, really impressive! But upon further investigation, something is off. The sensitivity is way off. Out of 10 children with Down Syndrome, the test only came back positive for 1, which yields a sensitivity of only only $0.1$. In other words, if a fetus actually is a affected, the test only has a $10\%$ chance of detecting it. That's not very comforting. 

This is a common problem with rare diseases. Since by far most individuals will not be diseased, a test that is good at predicting healthy individuals, but awful at predicting diseased individuals, will have a high accuracy, but such a test is not very desirable. Consider this last test for Down Syndrome: no test is performed, and we just always say the fetus is unaffected. Since 4800 out of 4810 fetuses were unaffected, we have an accuracy of $\frac{4800}{4810} = 0.998$. Pretty impressive accuracy rate, absolutely useless test...

## Example: positive/negative predictive value {-}

The specificity is the answer to the question "what is the probability the test will be correct when the patient is actually healthy?" This is of course a very important thing to know, and if this probability is very low, the test might not be particularly useful. However, a just as important, and sometimes more relevant, measure is the *negative predictive value*. This relates to the question "what is the probability the patient is actually healthy when the test comes back negative?" 

Similarly, we can talk about the *positive predictive value*. Where the sensitivity is the probability that the test is positive if the patient has the disease, the positive predictive value is the probability that a patient has the disease if the test comes back positive. 

Let us again consider the Down Syndrome data. Since the negative predictive value is the probability a child is healthy given the test was negative, it calculated as the proportion of children with negative tests that actually were healthy. So,

$$
\begin{align}
  \text{Positive Predictive Value} &= P(\text{child healthy } | \text{ test negative}) \\
                                   &= \frac{4449}{4450} \\
                                   &= 0.999.
\end{align}
$$

Similarly, since the negative positive predictive value is the probability a child has Down Syndrome given the test was positive, it is calculated as the proportion of children with positive tests that actually has Down Syndrome. So, 

$$
\begin{align}
  \text{Negative Predictive Value} &= P(\text{child diseased } | \text{ test positive}) \\
                                   &= \frac{9}{360} \\
                                   &= 0.025.
\end{align}
$$

## Bayes' Theorem

We have seen a few examples of some very useful and meaningful quantities that are actually conditional probabilities. We've seen how we, in general, calculate these conditional probabilities, but only in a setting where we know everything. The following theorem (i.e. very big and important result) provides a powerful way of finding conditional probabilities, and it also provides a very useful connection between conditional probabilities, and marginal probabilities (i.e. probabilities that are not conditional).

```{theorem, name = "Bayes' Theorem", label = bayes, echo = TRUE}
Bayes' Theorem simply states that $P(A | B) = \frac{P(A \text{ and } B)}{P(B)}$. Since $P(A \text{ and } B) = P(B \text{ and } A)$, this gives us that $P(B | A)P(A) = P(A \text{ and } B)$, so $P(A | B) = \frac{P(B | A)P(A)}{P(B)}$. Especially the latter formulation is very powerful, as we shall see in this next example.
```

### Example: positive predictive value from sensitivity{-} 

This allows us to calculate the positive predictive value using the sensitivity of a test, the prevalence of the disease we're testing for, and how often the test itself is positive (regardless of patient status). 


## Independence 

One of the big ones in statistics in general is the concept of independence. When things are independent, all the math simplifies a great deal, which is the main reason why a lot of the methods we will consider later on are based on the assumption that observations are independent of one another. 

Loosely speaking, two events are said to be *independent* if knowledge about one of the events does not provide any information about the other. I.e. if I ask you what the probabilitity of event A happening is before and after I tell you whether event B happened or not, your answers should be the same. 

### Example: independent events {-}

Event A: I walk around Madison one day, stop a random stranger, and ask: "are you taller than 6ft?"

Event B: I flip a coin, and it comes up tails.

Events A and B are independent. The probability that a random person is taller than 6ft is not altered by the fact that a coin flip comes up tails. 

### Example: dependent events {-}

Event A: I walk around Madison one day, stop a random stranger, and ask: "are you taller than 6ft?"

Event B: The random stranger I stop is male. 

Events A and B are NOT independent. The probability a random stranger is taller than 6ft is about 0.16 if the person is male, but less than 0.01 if the person is female.^[loosely based on data from https://dqydj.com/height-percentile-calculator-for-men-and-women/] So the probability of event A being 'yes' depends on the outcome of event B. Therefore, they are not independent. 

---

We will work with two definitions of independence. (Fortunately, they are equivalent, i.e. if one holds, the other holds.)

```{definition, echo = TRUE}
Two events are independent if and only if $P(A \text{ and } B) = P(A) P(B)$.
```

```{definition, echo = TRUE}
Two events are independent if and only if $P(A | B) = P(A)$ AND $P(B | A) = P(B)$. 
```

### Example {-}



# Random Variables and Distributions

So far in this section, we've talked about probabilities, different ways of thinking about probabilities, and a bit about how to work with probabilities. In this section we will introduce a more formal framework for how to think about and handle uncertain events. By making a few assumptions about how things behave, we can calculate probabilities of events without observing them.

## Random Variables

A *random variable* is a variable where the value is not guarenteed in advance, but can take different values. 

### Examples: random variables {-}

Define a variable $X$ to be the outcome of a coin flip. Now, before we flip the coin, we do not know what value $X$ will take on -- it could be "heads" or it could be "tails". Once we flip the coin and observe the outcome, we say that we have a *realization* of the random variable $X$. 

Another example would be if we let $Y$ be the height of a randomly chosen US adult. We don't know exactly what value it is, but we do know a few things about it. For example, it is much more likely to be around 5.5ft than it is to be around 7ft or below 4ft. When we do finally randomly select a US adult, and measure their height, we get a realization of this random variable. 

A third example is if we let $Z$ denote the diabetes status of a randomly chosen US adult. This could take the values healthy, type I, or type II, and each will happen with some probability. 

--- 

As the example above was meant to illustrate, a random variable can really be anything you'd like. And whenever we talk about a random variable, we also talk about the probability of certain outcomes. If we can define a way to calculate probabilities of different outcomes of the random variable, we call this the *distribution* of the random variable. 

Recall previously we talked about two kinds of variables: discrete and continuous variables. Likewise, we can consider both discrete and continuous *random* variables. Depending on the kind of random variable we're discussing, defining it's distribution is handled slightly differently. When we consider discrete random variables, its distribution is defined by specifying the probability of every single possible outcome. There are two things that are important to remember:

1. all probabilities must be between $0$ and $1$,
2. the sum of all probabilities must add up to $1$. 

The second point above is important, and is sometimes super handy when trying to calculate probabilities of certain complicated events. The intuition behind it is pretty simple: something must happen. So the probability that something happens is $1$. 

### Examples: discrete distributions {-}

Consider $X$ the outcome of a coin flip. The outcome of this can be one of two things: heads or tails. Now, let us pretend that this particular coin is NOT fair, i.e. it is not 50/50. Maybe the probability of getting tails is 0.4. Maybe the probability of getting heads is 0.1. For now, let the probability of getting heads be $p$, some number between 0 and 1. Then the probability of $X$ coming up as heads is $p$, and the probability it comes up as tails is $1-p$, since the sum of all probabilities has to be $1$. We write $P(X = \text{heads}) = p$ and $P(X = \text{tails}) = 1-p$. This is the distribution of $X$. In this case, all we need is the probability of the two outcomes. 

Another example: let $X$ be the marital status of a randomly chosen participant from the SHOW data. 

---

The examples above all consider discrete random variables. As already mentioned, the approach for continuous random variables is a bit different. For the distribution of a continuous random variable, we need to specify a curve for which the area under the curve is $1$. When we talk about probabilities of events that relate to the correpsonding random variable, we talk about areas under the curve.

### Examples: continuous distributions {-}

```{r}
full_SHOW <- read_csv("../data/SHOW.csv") %>% 
  rename(marital = "DMQ040")

ggplot(full_SHOW, 
       aes(x = ANT_MEAS_HEIGHT_CM)) + 
  geom_histogram(aes(y = ..density..),
                 bins = 35) + 
  stat_function(fun = dnorm, 
                args = list(mean = mean(full_SHOW$ANT_MEAS_HEIGHT_CM, na.rm = T),
                            sd = sd(full_SHOW$ANT_MEAS_HEIGHT_CM, na.rm = T)))
```

## Properties of Random Variables

When we talk about random variables, there is a great deal of uncertainty involved, since (by design) we do not know exactly what values the random variables will take after a conducted experiment. Similarly, we cannot be sure that repeating an experiment results in the same outcomes of the random variables simply since they are, as the name strongly implies, random. However, if we have some information about the random variable we're interested in, we can talk about some very important features of the random variable. The two we will talk about here are the *expected value* and *variance/standard deviation* of random variables. 

These two concepts can be a bit hard to wrap ones head around at first, but as we talk about them over and over agian, hopefully you will realize that they are not as abstract as they might first seem.

### Expected Value of Random Variables {-}

The expected value of a random variable is, intuitively, the long run average. I.e. if we repeat an experiment **an infinite number of times**, we can determine the expected value of a random variable as the average of all the realizations of said random variable. As an example, if we consider the random variable $X$ that is $0$ if a coin flip comes up heads, and $1$ if it comes up tails, we can imagine flipping a coin an infinite number of times, and calculating the average. The result would be that the expected value of $X$ is $0.5$. We write $E(X) = 0.5$. 

Since the expected value can be thought of as the long run average, it is in some sense the value that the outcomes of the random variable are going to be centered around. 

Note: the expected value is also often referred to as the *mean* value. 

For any discrete random variable where we know the distribution, we can find the expected value in the following way: $E(X) = x_1 \cdot P(X = x_1) + ... + x_n P(X = x_n) = \sum_{i=1}^n x_i P(X = x_i)$.^[**Note**: the symbol $\sum$ simply means "sum". So, when we write $\sum_{i=1}^n ...$ it simply means "take the expression ..., plug in the value when $i=1,2,3,...,n$, and then add them up". Example: $\sum_{i=1}^5 i = 1 + 2 + 3 + 4 + 5 = 15$. Example 2: if $x_1 = 1, x_2 = 6, x_3 = -2.9$, then $\sum_{i = 1}^3 x_i = 1 + 6 - 2.9 = 4.1$.]


#### Example: expected value of discrete random variable {-}

Let $X$ be a discrete random variable the can take the values $1,2,6$, and $12$. Let the probabilities of each outcome be as follows:

```{r}
mean_discrete_example <- tibble(x = c(1,2,6,12), `P(X = x)` = c(0.2, 0.1, 0.6, 0.1))
pander::pander(mean_discrete_example)
```

Then we can calculate the expected value of $X$: 

$$
\begin{align}
  E(X) &= \sum_{i = 1}^4 x_i P(X = x_i) \\
       &= 1 \cdot P(X = 1) + 2 \cdot P(X = 2) + 6 \cdot P(X = 6) + 12 \cdot P(X = 12) \\
       &= 1\cdot 0.2 + 2\cdot 0.1 + 6 \cdot 0.6 + 12 \cdot 0.1 \\
       &= 5.2.
\end{align}
$$

So what does this mean? It means that if we perform an experiment that results in a realization of the random variable $X$ many, many, many times, the average of all outcomes is going to be close to $5.2$.  

---

#### Example: expected value of a continuous random variable {-}

In the continuous case, actually calculating the expected value isn't as easy as in the discrete case. Remember, when we specify a discrete distribution, we specify the probability of each possible outcome. When we specify a continuous distribution, we specify a curve over all the possible outcomes, and probabilities of specific events correspond to areas under the curve. This also means that it is impossible to use a formula like the one introduced for the discrete case above. Fortunately, the intuition is the same. The expected it the long run average. 


---

#### Rules for working with expected values {-}

Sometimes, it is very beneficial to be able to transform a random variable, or combine several random variables, into a new one, and work with that new random variable. Fortunately, dealing with the expected value of a large number of such transformations is pretty simple. 

First, let's imagine we have a random variable $X$ with mean $E(X)$, and another random variable $Y$ with mean $E(Y)$. Perhaps we are interested in the sum of the two, so we construct a new random variable $Z = X + Y$.^[Example: maybe we sent out a survey to a bunch of households asking for the income of each adult in the household ($X$ and $Y$), and now we want to combine the two into a single total household income ($Z = X + Y$).] Finding the expected value of $Z$ is really simple: $E(Z) = E(X + Y) = E(X) + E(Y)$. In words: the expected value of a sum of random variables is simply the sum of expected values. 

Another example: maybe we want to scale the outcome of the random variable $X$ by a constant $a$, and then consider the new random variable $Y = a\cdot X$.^[Example: maybe $X$ is the total household income found from a survey in Europe where the currency is Euro. We want to compare this to our study of household incomes in the U.S., but to do so we have to convert from Euro to US Dollars. Here, $X$ is the household income in Euro, $a$ the exchange rate from Euro to US Dollars, and $Y$ the household income in US Dollars.] Again, finding the expected value of the new random variable $Y$ is really simple: $E(Y) = E(aX) = a\cdot E(X)$. 

One final thing I want to mention here: the expected value of a constant will always be the constant itself. Hopefully, this doesn't come as too much of a shock. The expected value is what we would *expect* from a random variable. If something is constant, it means it never changes, so we *expect* it to stay the same. So, if $a$ is a constant, $E(a) = a$. This can be combined with the first rule we talked about to give us that $E(X + a) = E(X) + E(a) = E(X) + a$. 


### Variance/Standard Deviation of Random Variables {-}

Where the expected value of a random variable tells us something about where the outcomes of the random variable tend to be located, the next measures we'll be looking at tell us something about how spread out the outcomes will be around the expected value.

**Note**: most textbooks handle the variance and standard deviations as two distinct things. I don't like that. They are virtually two sides of the same coin, and I will deliberately handle the two at the same time. My reasoning for this is that, at least in my head, these two measures try to convey the same message, but to two different audiences. I will elaborate on this later, but try to keep in mind that these two measures are almost the same. 

The *variance* of a random variable is a measure that tells us how much we expect the outcome of said random variable to vary from the expected value. As with the expected value, it is relatively simple to calculate this when we are dealing with simple discrete random variables. Let's say that we observe $n$ outcomes of a random variable $X$, $x_1, x_2, ..., x_n$. Then the variance of this sample of outcomes is $\Var(X) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2$, where $\bar{x}$ is the average of the values. If we were to break down this equation to better understand what's going on, we might do that in the following way:

1. It almost has the form $\frac{1}{n}\sum_{i=1}^n ...$, which is the form of an average. So, intuitively, we can think of this as an average
2. The values we take the average of, $(x_i - \bar{x})^2$, are representative of the distance from each observation to the average of the observations...
3. ... except, we square the distances. We do this because we want this measure to be representative of the variation of the data, and so we cannot allow positive and negative differences to cancel. Example: if we didn't square the differences, the sample of observation $1,2,3$ would have variance $0$, but clearly there is some variation in the sample -- not all observations are the same. 

So, loosely speaking, the variance is "a measure of averaged distances from observations to the sample average". 

#### Rules for working with variance {-}

Working with the variance of random variables is not quite as simple as working with the expected value. This is due to the fact that the expected value is a simple average, whereas the variance is an average of squared differences. The result is the following set of rules: if $X$ and $Y$ are random variables, and $a$ is some fixed constant, then 

1. $\Var(a\cdot X) = a^2 \Var(X)$,
2. $\Var(a) = 0$,
3. if (and ONLY if) $X$ and $Y$ are independent: $\Var(X+Y) = \Var(X) + \Var(Y)$.

Combining (1) and (2) above tells us that, if $X$ and $Y$ are independent, then $\Var(X - Y) = \Var(X + (-Y)) = \Var(X) + \Var(-Y) = \Var(X) + (-1)^2 \Var(Y) = \Var(X) + \Var(Y)$. Don't forget this!!

#### So what about that standard deviation? {-}

So far we've talked about the variance, a bit about how to interpret it, and how to work with it for multiple random variables. But what about that other thing mentioned above, the standard deviation? 

The standard deviation of a random variable is simply the square root of the variance: $\SD(X) = \sqrt{\Var(X)}$. The

### Things to remember when working with random variables {-}

When working with random variables, $X$ and $Y$, these are the important rules:

1. $E(X + Y) = E(X) + E(Y)$,
2. if $a$ is some fixed number, $E(a\cdot X) = a\cdot E(X)$,
3. if $a$ is some fixed number, $\Var(a \cdot X) = a^2 \Var(X)$,
4. **IF** $X$ and $Y$ are independent, $\Var(X + Y) = \Var(X) + \Var(Y)$,
5. **IF** $X$ and $Y$ are independent, $\Var(X - Y) = \Var(X) + \Var(Y)$. 

Things people often forget:

1. $E(X\cdot Y) \neq E(X)E(Y)$,
2. $E\left(\frac{X}{Y}\right) \neq \frac{E(X)}{E(Y)}$,
3. $\Var(X+Y) \neq \Var(X) + \Var(Y)$ if $X$ and $Y$ are not independent,
4. $\Var(X - Y) \neq \Var(X) - \Var(Y)$.

## A Few Important Distributions



### The Bernoulli Distribution {-}

In the first example above, we consider flipping a coin. Such an experiment, i.e. one with only two possible outcomes, is often referred to as a *Bernoulli experiment*, and the random variable $X$ is referred to as a *Bernoulli random variable*. The "probability of success" (you get to pick our favorite outcome as a success) is often denoted $p$. As a shorthand for such a random variable, we write $X \sim \text{Bernoulli}(p)$, which is read as "$X$ follows a Bernoulli distribution with probability parameter $p$" or "$X$ is Bernoulli distributed with parameter $p$". Phrases like these can sometimes sound scary and complex, but all it means is that the random variable $X$ can only take on two different outcomes, and the probability of $X$ being one of the two outcomes is $p$, the probability of it being the other is $1-p$. (Important note: remember that the sum of all probabilities has to be $1$, so if the probability of one outcome is $p$, and there are only two possible outcomes, then the probability of the other outcome must be $1-p$. This way of thinking is something we will use over and over again.)

### The Binomial Distribution {-}

Often times we are interested in things that can be viewed as a sum of Bernoulli random variables. Let's say we have $n$ independent (i.e. the outcome of one doesn't say anything about the rest) Bernoulli random variables ($X_1$, $X_2$, ..., $X_n$), all with probability of success $p$, and are interested in the sum of those $n$ variables $Y = X_1 + X_2 + ... + X_n$. For this to make sense, we let $X_i$ be $1$ if the corresponding "experiment" is a success, and $0$ if it is a failure. Now, we can think of the random variable $Y$ as either (1) the sum of independent Bernoulli random variables, or (2) the number of successes among $n$ independent trials with binary outcomes. It is this latter interpretation that makes the random variable $Y$ interesting. 

Let's think for a second about what possible values $Y$ can take. If all $n$ Bernoulli experiments happen to come out as failures, then all $X_i$'s are $0$'s, and so $Y$ will also be $0$. The other extreme is if all $n$ Bernoulli experiments are successes, then all $X_i$'s are $1$'s, and $Y$ will be the sum of $n$ $1$'s, so $Y$ will be $n$. These are simply the two extremes - any number of the $X_i$'s can be $1$'s, so $Y$ can end up being any integer between $0$ and $n$, both included. The most likely scenarios are the integers closest to the middle. 

Since $Y$ is simply a sum of very simple random variables, namely Bernoulli random variables, we can with very simple tools dive deeper, and try to explore what the distribution of a Binomial random variable looks like. There are two ways of doing this: (1) do the math, or (2) flip $n$ coins an infinite number of times and see how often the number of heads is each of the possible outcomes. Let's start with the latter. 

```{r binomial_example, cache = TRUE}
binom_n <- 100000
binom_n_print <- format(binom_n, scientific = F)

n <- 10

binom_outcomes <- tibble(i = 1:binom_n,
                         Numeric = map(i, function(...) as.numeric(rbernoulli(n))),
                         HTs = map_chr(Numeric, function(x) if_else(x == 1, "H", "T") %>% 
                                         paste(collapse = ",")),
                         Y = map_dbl(Numeric, sum)) %>% 
  select(i, HTs, Numeric, Y)
```

Since it's impossible to flip $n$ coins (for what is $n$?), we have to pick a real integer. Let's pick $`r n`$. Similarly, it's impossible to flip $`r n`$ coins an infinite number of times, so let's just do it a bunch of times (i.e. $`r `$). What we are about to do is repeat an experiment (flip $`r n`$ coins) many, many ($`r binom_n_print`$) times. The first time we perform this experiment, we see `r binom_outcomes$HTs[1]`. When we translate this to $0$ and $1$, it looks like `r paste(binom_outcomes$Numeric[[1]], collapse = ",")`. So, the value of the binomial variable $Y$ is `r binom_outcomes$Y[1]`. The results of all $`r n`$ experiments are shown in the table below.

```{r cache = TRUE}
DT::datatable(binom_outcomes %>% select(-i))
```

Now we can get a pretty good estimate of the distribution of $Y$. Recall, the distribution of a random variable is simply the probabilities of each possible outcome. The probability of a particular outcome, say $Y = 2$, is the long run proportion of experiments that result in that outcome. So, $P(Y = 2) = \frac{\text{# experiments with } Y = 2}{\text{# experiments}} = \frac{`r sum(binom_outcomes[["Y"]] == 2)`}{`r binom_n`} = `r round(sum(binom_outcomes[["Y"]] == 2)/binom_n, digits = 5)`$. If we do this for every possible value of $Y$, we get something that looks like the following:

```{r}
binom_outcomes %>% 
  janitor::tabyl(Y) %>% 
  rename(y = Y,
         `Estimated Probability` = percent,
         `# experiments with Y = y` = n) %>% 
  pander::pander()
```

We see that the most probable outcomes are around the middle (4,5,6) with proportions above 0.20. 

Another popular way of displaying this is using a histogram:

```{r}
binom_outcomes %>%  
  janitor::tabyl(Y) %>% 
  ggplot(aes(x = Y, y = n)) + 
    geom_bar(stat = 'identity', width = 1, col = 'black') +
    scale_x_continuous(breaks = c(1:10)) +
    theme_bw()
```

When viewing this, the probability of a given outcome can be interpreted as the area of the corresponding bar divided by the total area. 

As mentioned earlier, the distribution of a binomial random variable can also be calculated mathematically. We won't go into the details here, but take a look at the calculated probabilities below, and compare them to the estimates we got by flipping $`r n`$ coins $`r binom_n_print`$ times. 

```{r}
binom_outcomes %>% 
  janitor::tabyl(Y) %>% 
  mutate(Probability = dbinom(x = Y, size = 10, prob = 0.5)) %>% 
  rename(y = Y,
         `Estimated Probability` = percent,
         `# experiments with Y = y` = n) %>% 
  pander::pander()
```

### Normal Distribution {-}

The normal distribution is 

### t-distribution {-}

### Other Distribution

The four distributions above are the ones we'll consider, but there are many, many more out there. Here are a few examples.

#### Poisson Distribution {-}

The Poisson distribution is often used for counting things, such as the number of patients showing up in a clinic during a specified time period. It is a discrete distribution that only returns integer values. It depends on only one parameter which is often referred to as the rate parameter. It is displayed below with a few different values of the rate.

```{r}
tibble(lambda = c(1, 5, 10),
       n = list(c(0:20)),
       p = map2(n, lambda, function(x,y) dpois(x, y))) %>% 
  unnest() %>% 
  ggplot(aes(x = n, y = p, fill = as.character(lambda))) + 
    geom_histogram(stat = 'identity', position = "identity",
                   alpha = 0.75, width = 1, col = 'grey50') + 
    labs(y = 'Probabilities', fill = "Rate parameter") +
    theme_bw()
```

#### Exponential Distribution {-}

The exponential distribution is often used for wait times. It is a continuous distribution that depends on a single parameter, which is also called the rate parameter. 

```{r}
tibble(x = list(seq(0, 2, by = 0.05)),
       Rate = c(1,5,10),
       p = map2(x, Rate, function(x,y) dexp(x, y))) %>% 
  unnest() %>% 
  mutate(Rate = factor(Rate, levels = c(1, 5, 10))) %>% 
  ggplot(aes(x = x, y = p, fill = Rate)) + 
    geom_area(col = 'grey50', alpha = 0.75) + 
    facet_grid(~Rate) + 
    scale_y_continuous('', expand = expand_scale(mult = c(0,0), add = c(0, 2))) 
```




# Estimators and their distributions

- simulations for explanation
- as concrete and intuitive as possible

# Central Limit Theorem.

- Probability Sampling (Simple random/Systematic/Stratified Sampling)
- Non-probability Sampling (Convenience/Quota Sampling)
- Basic Concepts
    
