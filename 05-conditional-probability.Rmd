# Conditional Probability

So far, we have talked about probabilities in a context where no additional information is available about the experiment. This is of course not always the case, and also not always what we are interested in. 

A useful concept in these cases is the concept of *conditional probabilities*. In a nutshell, conditional probabilities deal with the chances of something happening given something else has already happened. If we consider two events, $A$ and $B$, then we write $P(A | B)$ for the conditional probability of $A$ given that $B$ has happened, and read it as "the (conditional) probability of $A$ given $B$". 

## Example: roll a die 

Previously, we considered the probabilities associated with the roll of a die. We found that the probability of rolling a six is $\frac{1}{6}$. What if we somehow knew that the outcome turned out to be an even number, but simply didn't know which even number? Well, using this information, we know there are only three possible outcomes, namely $2,4,6$. They are all equally likely, so using definition \@ref(def:prob-def-1), we find that the probability of rolling a six given the roll comes up even is 

$$\left .P(\text{roll a } 6\ \right|\ \text{roll is even}) = \frac{1}{3}.$$

## Example: disease status 

In the last section we found $P(\text{mild depression}) \approx 0.134$. Let us try to calculate the conditional probability of having a mild depression *given* the subject is divorced. The way to do this is to first create the two way contingency table: 

```{r}
dep_sev_by_marital <- full_show %>% 
  janitor::tabyl(depression_severity, marital) %>% 
  janitor::adorn_totals(where = c("row", "col")) %>% 
  rename(`Depression Severity` = depression_severity)

dep_sev_by_marital %>% 
  pander::pander(split.table = Inf)
```

To find the conditional probability, you basically narrow down the universe you operate in. Instead of asking "how many individuals have mild depression out of all individuals?" you ask "how many individuals have mild depression out of **individuals that are divorced**?". So, in other words, all you worry about is the column in the table corresponding to the divorced subjects. We estimate the conditional probability of having a mild depression given the subject is divorced as $P(\text{mild depression} | \text{divorced}) = \frac{76}{416} \approx `r round(76/416, digits = 3)`$. 


```{r}
emph_col <- which(str_detect(colnames(dep_sev_by_marital), "Divorced"))

dep_sev_by_marital %>% 
  pander::pander(split.table = Inf, emphasize.strong.cols = emph_col)
```


## Example: Sensitivity/specificity 

Two important examples of conditional probabilities are the so-called sensitivity and specificity. These are particularly useful when discussing the accuracy of screening tests. 

The *sensitivity* of a test is the *true positive rate* (or fraction). That is, out of the tests performed on individuals with the disease of interest, how many come out positive. I.e. $\text{sensitivity} = P(\text{test positive}\ |\ \text{individual diseased})$. 

Similarly, the *specificity* of a test is the *true negative rate* (or fraction), i.e. the proportion of tests performed on healthy individuals that come out negative: $\text{specificity} = P(\text{test negative}\ |\ \text{individual healthy})$. 

It is also often useful to consider the *false positive rate* (FPR) and *false negative rate* (FNR). These are defined as follows:

\begin{align*}
  \text{FPR} &= P(\text{test positive}\ |\ \text{individual healthy}), \\
  \text{FNR} &= P(\text{test negative}\ |\ \text{individual diseased}). \\
\end{align*}

Let's consider a concrete example. Below is table 5-5 from @ls. This table shows the results of screenings of 4810 pregnant women to assess if their fetus is likely to have Down Syndrome. After birth, it is determined if the child actually has Down Syndrome, provided a ground truth that we can check our screening method against. Ideally, the test is positive for all kids with Down Syndrome, and negative for alld kids without Down Syndrome. 

```{r}
prenatal_screening <- tibble(`Fetus Status` = c('Affected', 'Affected', 'Unaffected', 'Unaffected'),
                             `Screening Test Result` = factor(c('Positive', 'Negative', 'Positive', 'Negative'),
                                                              levels = c('Positive', 'Negative')),
                             n = c(9, 1, 351, 4449)) %>% 
  spread(`Fetus Status`, n)

prenatal_screening_DT <- prenatal_screening %>% 
  janitor::as_tabyl() %>% 
  janitor::adorn_totals(where = c('row', 'col')) %>% 
  DT::datatable(options = list(dom = "t", ordering = FALSE), 
                rownames = FALSE,
                width = 10)

prenatal_screening_DT
```

Let us calculate the specificity, sensitivity, FNR, and FPR:

\begin{align*}
  \text{specificity} &= P(\text{test negative}\ |\ \text{child healthy}) \\
                     &= \frac{\text{number of negative tests among healthy children}}{\text{number of healthy children}} \\
                     &= \frac{4449}{4800} = 0.927 \\
                     & \\
  \text{sensitivity} &= P(\text{test positive}\ |\ \text{child has Down Syndrome}) \\
                     &= \frac{\text{number of positive tests among children with Down Syndrome}}{\text{number of children with Down Syndrome}} \\
                     &= \frac{9}{10} = 0.9 \\
                     & \\
  \text{FPR} &= P(\text{test positive}\ |\ \text{individual healthy}) \\
             &= \frac{\text{number of positive tests among healthy children}}{\text{number of healthy children}} \\
             &= \frac{351}{4800} = 0.073 \\
             & \\
  \text{FNR} &= P(\text{test negative}\ |\ \text{individual diseased}) \\
             &= \frac{\text{number of negative tests among children with Down Syndrome}}{\text{number of children with Down Syndrome}} \\
             &= \frac{1}{10} = 0.1.
\end{align*}

We see that the test has some very desirable attributes, in high specificity AND high sensitivity. At this point, some might stop and wonder for a second: the end goal is to determine if the test is accurate, so why don't we just calculate the accuracy of the test? I.e. what's wrong at simply looking at the number of corret test results out of the total number of tests? Let's take a look.

\begin{align*}
  \text{test accuracy} &= \frac{\text{number of correct results}}{\text{number of tests performed}} \\
                       &= \frac{9 + 4449}{4810} \\
                       &= \frac{4458}{4810} = 0.927
\end{align*}


That's pretty impressive. The test has an accuracy rate of almost $93\%$, i.e. almost $93\%$ of tests yield the correct result. Now, let us consider a different test for the same disease. Tested on the same 4810 women, and pretend it yields the following results:

```{r}
tibble(`Fetus Status` = c('Affected', 'Affected', 'Unaffected', 'Unaffected'),
                             `Screening Test Result` = factor(c('Positive', 'Negative', 'Positive', 'Negative'),
                                                              levels = c('Positive', 'Negative')),
                             n = c(1, 9, 351, 4449)) %>% 
  spread(`Fetus Status`, n) %>% 
  arrange(`Screening Test Result`) %>% 
  janitor::as_tabyl() %>% 
  janitor::adorn_totals(where = c('row', 'col')) %>% 
  DT::datatable(options = list(dom = "t", ordering = FALSE), 
                rownames = FALSE,
                width = 10)
```


Now, the accuracy rate of this test is $\frac{1 + 4449}{4810} = 0.925$, i.e. almost the same as the first test. That's, again, really impressive! But upon further investigation, something is off. The sensitivity is way off. Out of 10 children with Down Syndrome, the test only came back positive for 1, which yields a sensitivity of only only $0.1$. In other words, if a fetus actually is a affected, the test only has a $10\%$ chance of detecting it. That's not very comforting. 

This is a common problem with rare diseases. Since by far most individuals will not be diseased, a test that is good at predicting healthy individuals, but awful at predicting diseased individuals, will have a high accuracy, but such a test is not very desirable. Consider this last test for Down Syndrome: no test is performed, and we just always say the fetus is unaffected. Since 4800 out of 4810 fetuses were unaffected, we have an accuracy of $\frac{4800}{4810} = 0.998$. Pretty impressive accuracy rate, absolutely useless test...

## Example: positive/negative predictive value 

The specificity is the answer to the question "what is the probability the test will be correct when the patient is actually healthy?" This is of course a very important thing to know, and if this probability is very low, the test might not be particularly useful. However, a just as important, and sometimes more relevant, measure is the *negative predictive value*. This relates to the question "what is the probability the patient is actually healthy when the test comes back negative?" 

Similarly, we can talk about the *positive predictive value*. Where the sensitivity is the probability that the test is positive if the patient has the disease, the positive predictive value is the probability that a patient has the disease if the test comes back positive. 

Let us again consider the Down Syndrome data. Since the negative predictive value is the probability a child is healthy given the test was negative, it calculated as the proportion of children with negative tests that actually were healthy. So,

\begin{align*}
  \text{Positive Predictive Value} &= P(\text{child healthy } | \text{ test negative}) \\
                                   &= \frac{4449}{4450} \\
                                   &= 0.999.
\end{align*}


Similarly, since the negative positive predictive value is the probability a child has Down Syndrome given the test was positive, it is calculated as the proportion of children with positive tests that actually has Down Syndrome. So, 


\begin{align*}
  \text{Negative Predictive Value} &= P(\text{child diseased } | \text{ test positive}) \\
                                   &= \frac{9}{360} \\
                                   &= 0.025.
\end{align*}


## Bayes' Theorem

We have seen a few examples of some very useful and meaningful quantities that are actually conditional probabilities. We've seen how we, in general, calculate these conditional probabilities, but only in a setting where we know everything. The following theorem^[to those who are not familiar with the math jargon: theorem = very big and important result] provides a powerful way of finding conditional probabilities, and it also provides a very useful connection between conditional probabilities, and marginal probabilities (i.e. probabilities that are not conditional).

```{theorem, name = "Bayes' Theorem", label = bayes, echo = TRUE}
Bayes' Theorem simply states that 

$$
P(A | B) = \frac{P(A \text{ and } B)}{P(B)} (\#eq:bayes1),
$$

or equivalently

$$
P(A | B) = \frac{P(B | A)P(A)}{P(B)} (\#eq:bayes2)
$$

```

Since $P(A \text{ and } B) = P(B \text{ and } A)$, equation \@ref(eq:bayes1) gives us that $P(B | A)P(A) = P(A \text{ and } B)$, and so equation \@ref(eq:bayes2) follows by plugging this into the numerator in equation \@ref(eq:bayes1). 

Especially the latter formulation is very powerful, as we shall see in this next example.

### Example (5.8 in @ls): positive predictive value from sensitivity

Bayes' Theorem allows us to calculate the positive predictive value using the sensitivity of a test, the prevalence of the disease we're testing for, and how often the test itself is positive (regardless of patient status). 

Consider a situation where a disease is really rare with a prevalence of $0.2\%$ (i.e. $2$ in $1000$ individuals have the disease). A screening test for this disease has a reported sensitivity of $85\%$, comes back positive $8\%$ of the time, and negative $92\%$ of the time. 

We would like to know what the positive predictive value is, i.e. $P(\text{patient has the disease} | \text{screen positive})$. Using Bayes' rule, we know 


\begin{align*}
  &P(\text{patient has the disease} | \text{screen positive}) = \\
  &\hspace{2cm} \frac{P(\text{screen positive} | \text{patient has the disease}) \cdot P(\text{patient has the disease})}{P(\text{screen positive})}
\end{align*}


Notice: $P(\text{screen positive} | \text{patient has the disease})$ is exactly the sensitivity, $P(\text{patient has the disease})$ is the prevalence, and $P(\text{screen positive})$ is the probability the screening test comes back positive. We know all these probabilities, and so we can calculate the positive predictive value. 

\begin{align*}
  \text{PPV} &= P(\text{patient has the disease} | \text{screen positive}) \\
             &= \frac{P(\text{screen positive} | \text{patient has the disease}) \cdot P(\text{patient has the disease})}{P(\text{screen positive})} \\
             &= \frac{0.85 \cdot 0.002}{0.08} \approx `r round(0.85*0.002/0.08, digits = 3)`
\end{align*}

## Independence 

One of the big concepts in statistics in general is the concept of independence. When things are independent, all the math simplifies a great deal, which is the main reason why a lot of the methods we will consider later on are based on the assumption that observations are independent of one another. 

Loosely speaking, two events are said to be *independent* if knowledge about one of the events does not provide any information about the other. I.e. if I ask you what the probabilitity of event A before and after I tell you whether event B happened or not, your answers should be the same. 

### Example: independent events 

Event A: I walk around Madison one day, stop a random stranger, and ask: "are you taller than 6ft?"

Event B: I flip a coin, and it comes up tails.

Events A and B are independent. The probability that a random person is taller than 6ft is not altered by the fact that a coin flip comes up tails. 

### Example: dependent events 

Event A: I walk around Madison one day, stop a random stranger, and ask: "are you taller than 6ft?"

Event B: The random stranger I stop is male. 

Events A and B are NOT independent. The probability a random stranger is taller than 6ft is about 0.16 if the person is male, but less than 0.01 if the person is female.^[loosely based on data from https://dqydj.com/height-percentile-calculator-for-men-and-women/] So the probability of event A being 'yes' depends on the outcome of event B. Therefore, they are not independent. 

---

We will work with two definitions of independence. (Fortunately, they are equivalent, i.e. if one holds, the other holds.)

```{definition, label = 'ind1', echo = TRUE}
Two events are independent if and only if $P(A \text{ and } B) = P(A) P(B)$.
```

```{definition, label = 'ind2', echo = TRUE}
Two events are independent if and only if $P(A | B) = P(A)$ **AND** $P(B | A) = P(B)$. 
```

### Example: are "depression severity = mild depression" and "marital status = divorced" independent?

Let's say that $A = \text{subject is divorced}$ and $B = \text{subject is mildly depressed}$. For simplicity, we only consider subjects for which we know both marital status and depression severity, i.e. any subjects with missing data in one of the two variables have been removed.

The contingency table:

```{r}
full_show %>% 
  select(depression_severity, marital) %>% 
  mutate(depression_severity = ifelse(depression_severity == '[2] Mild depression', 'mild depression', 'not mild depression'), 
         marital = ifelse(marital == '[3] Divorced', 'Divorced', 'Not divorced')) %>% 
  filter(complete.cases(.)) %>% 
  janitor::tabyl(depression_severity, marital) %>% 
  janitor::adorn_totals(where = c('row', 'col')) %>% 
  rename(`Depression\nSeverity` = depression_severity) %>% 
  pander::pander()
```


Now, we can test for independence in two different ways: either using definition \@ref(def:ind1) or definition \@ref(def:ind2). Let's do both.

Using definition \@ref(def:ind1), we need to calculate three probabilities: 

\begin{align*}
  P(A \text{ and } B) &= P(\text{divorced and mildly depressed}) = \frac{76}{2273} \approx `r round(76/2273, digits = 4)` \\
  & \\
  P(A) &= P(\text{divorced}) = \frac{295}{2273} \approx `r round(295/2273, digits = 4)` \\
  & \\
  P(B) &= P(\text{mild depression}) = \frac{453}{2273} \approx `r round(453/2273, digits = 4)`
\end{align*}



Since $P(A)\cdot P(B) \approx `r round(295/2273, digits = 4)` \cdot `r round(453/2273, digits = 4)` \approx `r round(453/2273, digits = 4)*round(295/2273, digits = 4)`$, which is NOT the same as $P(A \text{ and } B) = `r round(76/2273, digits = 4)`$, these two events are not independent of each other.

Using definition \@ref(def:ind2), we need to calculate four probabilities. Two of them, $P(A)$ and $P(B)$, we already calculated. Let's calculate the remaining two:

\begin{align*}
  P(A | B) &= P(\text{divorced} | \text{mildly depressed}) = \frac{76}{453} \approx `r round(76/453, digits = 4)`\\
  & \\
  P(B | A) &= P(\text{mildly depressed} | \text{divorced}) = \frac{76}{295} \approx `r round(76/295, digits = 4)`
\end{align*}

As you can see, $P(A | B) \neq P(A)$ and $P(B | A) \neq P(B)$, so again, the conclusion is that the two events are not independent of one another. 

What if we instead let $A = \text{subject is male}$ and $B = \text{subject is married}$? The contingency table:


```{r}
full_show %>% 
  select(gender, marital) %>% 
  filter(complete.cases(.)) %>% 
  janitor::tabyl(gender, marital) %>% 
  janitor::adorn_totals(where = c('row', 'col')) %>% 
  pander::pander(split.tab = Inf)
```

Let us just check using definition \@ref(def:ind1):

\begin{align*}
  P(A \text{ and } B) &= P(\text{male and married}) = \frac{928}{3376} \approx `r round(928/3376, digits = 4)` \\
  & \\
  P(A) &= P(\text{male}) = \frac{1477}{3376} \approx `r round(1477/3376, digits = 4)` \\
  & \\
  P(B) &= P(\text{married}) = \frac{2075}{3376} \approx `r round(2075/3376, digits = 4)`
\end{align*}

So $P(A)\cdot P(B) \approx `r round(2075*1477/3376^2, digits = 4)`$, which is not very different from $P(A \text{ and } B)$. Seems reasonable to say that these events indeed are independent. 

---

While definition \@ref(def:ind1) is very useful when doing math, it might not make a whole lot of sense intuitively. Definition \@ref(def:ind2), on the other hand, aligns with our intuitive understanding of independence. 

Intuitively, two events, $A$ and $B$, are independent if $A$ provides no information about $B$, and vice versa. Now, if $A$ provides no information about $B$, then what would the difference between $P(B)$ and $P(B|A)$ be? Remember, the latter is basically the probability of $B$ if $A$ happens. But if $A$ and $B$ are independent, $A$ provides no information about $B$, so the probability of $B$ doesn't change if $A$ happens. 

