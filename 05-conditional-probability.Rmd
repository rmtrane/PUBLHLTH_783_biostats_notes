# Conditional Probability

So far, we have talked about probabilities in a context where no additional information is available about the experiment. This is of course not always the case, and also not always what we are interested in. 

A useful concept in these cases is the concept of *conditional probabilities*. In a nutshell, conditional probabilities deal with the chances of something happening given something else has already happened. If we consider two events, $A$ and $B$, then we write $P(A | B)$ for the conditional probability of $A$ given that $B$ has happened.

## Example: roll a die {-}

Previously, we considered the probabilities associated with the roll of a die. We found that the probability of rolling a six is $\frac{1}{6}$. What if we somehow knew that the outcome turned out to be an even number, but simply didn't know which even number? Well, using this information, we know there are only three possible outcomes, namely $2,4,6$. They are all equally likely, so using the probability of rolling a six given the roll comes up even is 

$$\left .P(\text{roll a } 6\ \right|\ \text{roll is even}) = \frac{1}{3}.$$

## Example: disease status {-}



## Example: Sensitivity/specificity {-}

Two important examples of conditional probabilities are the so-called sensitivity and specificity. These are particularly useful when discussing the accuracy of screening tests. 

The *sensitivity* of a test is the *true positive rate* (or fraction). That is, out of the tests performed on individuals with the disease of interest, how many come out positive. I.e. $\text{sensitivity} = P(\text{test positive}\ |\ \text{individual diseased})$. 

Similarly, the *specificity* of a test is the *true negative rate* (or fraction), i.e. the proportion of tests performed on healthy individuals that come out negative: $\text{specificity} = P(\text{test negative}\ |\ \text{individual healthy})$. 

It is also often useful to consider the *false positive rate* (FPR) and *false negative rate* (FNR). These are defined as follows:

\begin{align*}
  \text{FPR} &= P(\text{test positive}\ |\ \text{individual healthy}), \\
  \text{FNR} &= P(\text{test negative}\ |\ \text{individual diseased}). \\
\end{align*}

Let's consider a concrete example. Below is table 5-5 from @ls. This table shows the results of screenings of 4810 pregnant women to assess if their fetus is likely to have Down Syndrome. After birth, it is determined if the child actually has Down Syndrome, provided a ground truth that we can check our screening method against. Ideally, the test is positive for all kids with Down Syndrome, and negative for alld kids without Down Syndrome. 

```{r}
prenatal_screening <- tibble(`Fetus Status` = c('Affected', 'Affected', 'Unaffected', 'Unaffected'),
                             `Screening Test Result` = factor(c('Positive', 'Negative', 'Positive', 'Negative'),
                                                              levels = c('Positive', 'Negative')),
                             n = c(9, 1, 351, 4449)) %>% 
  spread(`Fetus Status`, n)

prenatal_screening_DT <- prenatal_screening %>% 
  janitor::as_tabyl() %>% 
  janitor::adorn_totals(where = c('row', 'col')) %>% 
  DT::datatable(options = list(dom = "t", ordering = FALSE), 
                rownames = FALSE,
                width = 10)

prenatal_screening_DT
```

Let us calculate the specificity, sensitivity, FNR, and FPR:

$$
\begin{align*}
  \text{specificity} &= P(\text{test negative}\ |\ \text{child healthy}) \\
                     &= \frac{\text{# negative tests among healthy children}}{\text{# healthy children}} \\
                     &= \frac{4449}{4800} = 0.927 \\
                     & \\
  \text{sensitivity} &= P(\text{test positive}\ |\ \text{child has Down Syndrome}) \\
                     &= \frac{\text{# positive tests among children with Down Syndrome}}{\text{# children with Down Syndrome}} \\
                     &= \frac{9}{10} = 0.9 \\
                     & \\
  \text{FPR} &= P(\text{test positive}\ |\ \text{individual healthy}) \\
             &= \frac{\text{# positive tests among healthy children}}{\text{# healthy children}} \\
             &= \frac{351}{4800} = 0.073 \\
             & \\
  \text{FNR} &= P(\text{test negative}\ |\ \text{individual diseased}) \\
             &= \frac{\text{# negative tests among children with Down Syndrome}}{\text{# children with Down Syndrome}} \\
             &= \frac{1}{10} = 0.1.
\end{align*}
$$

We see that the test has some very desirable attributes, in high specificity AND high sensitivity. At this point, some might stop and wonder for a second: the end goal is to determine if the test is accurate, so why don't we just calculate the accuracy of the test? I.e. what's wrong at simply looking at the number of corret test results out of the total number of tests? Let's take a look.

$$
\begin{align}
  \text{test accuracy} &= \frac{\text{# correct results}}{\text{# tests performed}} \\
                       &= \frac{9 + 4449}{4810} \\
                       &= \frac{4458}{4810} = 0.927
\end{align}
$$

That's pretty impressive. The test has an accuracy rate of almost $93\%$, i.e. almost $93\%$ of tests yield the correct result. Now, let us consider a different test for the same disease. Tested on the same 4810 women, let's pretend it yields the following results:

```{r}
prenatal_screening %>% 
  mutate(`Screening Test Result` = factor(c('Negative', 'Positive'), 
                                          levels = c('Positive', 'Negative'))) %>% 
  arrange(`Screening Test Result`) %>% 
  janitor::as_tabyl() %>% 
  janitor::adorn_totals(where = c('row', 'col')) %>% 
  DT::datatable(options = list(dom = "t", ordering = FALSE), 
                rownames = FALSE,
                width = 10)
```


Now, the accuracy rate of this test is $\frac{1 + 4449}{4810} = 0.925$, i.e. almost the same as the first test. That's, again, really impressive! But upon further investigation, something is off. The sensitivity is way off. Out of 10 children with Down Syndrome, the test only came back positive for 1, which yields a sensitivity of only only $0.1$. In other words, if a fetus actually is a affected, the test only has a $10\%$ chance of detecting it. That's not very comforting. 

This is a common problem with rare diseases. Since by far most individuals will not be diseased, a test that is good at predicting healthy individuals, but awful at predicting diseased individuals, will have a high accuracy, but such a test is not very desirable. Consider this last test for Down Syndrome: no test is performed, and we just always say the fetus is unaffected. Since 4800 out of 4810 fetuses were unaffected, we have an accuracy of $\frac{4800}{4810} = 0.998$. Pretty impressive accuracy rate, absolutely useless test...

## Example: positive/negative predictive value {-}

The specificity is the answer to the question "what is the probability the test will be correct when the patient is actually healthy?" This is of course a very important thing to know, and if this probability is very low, the test might not be particularly useful. However, a just as important, and sometimes more relevant, measure is the *negative predictive value*. This relates to the question "what is the probability the patient is actually healthy when the test comes back negative?" 

Similarly, we can talk about the *positive predictive value*. Where the sensitivity is the probability that the test is positive if the patient has the disease, the positive predictive value is the probability that a patient has the disease if the test comes back positive. 

Let us again consider the Down Syndrome data. Since the negative predictive value is the probability a child is healthy given the test was negative, it calculated as the proportion of children with negative tests that actually were healthy. So,

$$
\begin{align}
  \text{Positive Predictive Value} &= P(\text{child healthy } | \text{ test negative}) \\
                                   &= \frac{4449}{4450} \\
                                   &= 0.999.
\end{align}
$$

Similarly, since the negative positive predictive value is the probability a child has Down Syndrome given the test was positive, it is calculated as the proportion of children with positive tests that actually has Down Syndrome. So, 

$$
\begin{align}
  \text{Negative Predictive Value} &= P(\text{child diseased } | \text{ test positive}) \\
                                   &= \frac{9}{360} \\
                                   &= 0.025.
\end{align}
$$

## Bayes' Theorem

We have seen a few examples of some very useful and meaningful quantities that are actually conditional probabilities. We've seen how we, in general, calculate these conditional probabilities, but only in a setting where we know everything. The following theorem (i.e. very big and important result) provides a powerful way of finding conditional probabilities, and it also provides a very useful connection between conditional probabilities, and marginal probabilities (i.e. probabilities that are not conditional).

```{theorem, name = "Bayes' Theorem", label = bayes, echo = TRUE}
Bayes' Theorem simply states that $P(A | B) = \frac{P(A \text{ and } B)}{P(B)}$. Since $P(A \text{ and } B) = P(B \text{ and } A)$, this gives us that $P(B | A)P(A) = P(A \text{ and } B)$, so $P(A | B) = \frac{P(B | A)P(A)}{P(B)}$. Especially the latter formulation is very powerful, as we shall see in this next example.
```

### Example: positive predictive value from sensitivity{-} 

This allows us to calculate the positive predictive value using the sensitivity of a test, the prevalence of the disease we're testing for, and how often the test itself is positive (regardless of patient status). 


## Independence 

One of the big ones in statistics in general is the concept of independence. When things are independent, all the math simplifies a great deal, which is the main reason why a lot of the methods we will consider later on are based on the assumption that observations are independent of one another. 

Loosely speaking, two events are said to be *independent* if knowledge about one of the events does not provide any information about the other. I.e. if I ask you what the probabilitity of event A happening is before and after I tell you whether event B happened or not, your answers should be the same. 

### Example: independent events {-}

Event A: I walk around Madison one day, stop a random stranger, and ask: "are you taller than 6ft?"

Event B: I flip a coin, and it comes up tails.

Events A and B are independent. The probability that a random person is taller than 6ft is not altered by the fact that a coin flip comes up tails. 

### Example: dependent events {-}

Event A: I walk around Madison one day, stop a random stranger, and ask: "are you taller than 6ft?"

Event B: The random stranger I stop is male. 

Events A and B are NOT independent. The probability a random stranger is taller than 6ft is about 0.16 if the person is male, but less than 0.01 if the person is female.^[loosely based on data from https://dqydj.com/height-percentile-calculator-for-men-and-women/] So the probability of event A being 'yes' depends on the outcome of event B. Therefore, they are not independent. 

---

We will work with two definitions of independence. (Fortunately, they are equivalent, i.e. if one holds, the other holds.)

```{definition, echo = TRUE}
Two events are independent if and only if $P(A \text{ and } B) = P(A) P(B)$.
```

```{definition, echo = TRUE}
Two events are independent if and only if $P(A | B) = P(A)$ AND $P(B | A) = P(B)$. 
```

### Example {-}



