[
["index.html", "Biostatistics in Public Health Welcome!", " Biostatistics in Public Health Ralph Trane Fall 2019(last compiled: 2019-08-21) Welcome! Introduction to the biostatistics part of the course Expectations and structure Explanation of notes … "],
["intro.html", "1 Introduction to Biostatistics", " 1 Introduction to Biostatistics Words about biostats. (Chapter 4 in Sullivan 2017.) B Simulating Data "],
["discrete.html", "2 Discrete Data 2.1 Categorical data 2.2 How to describe categorical data 2.3 Ordinal Data 2.4 How to visualize discrete data", " 2 Discrete Data A variable is called a discrete variable if the possible values of the variable are countable, that is if you can count them. Note that a discrete variable can technically have an infinite number of possible outcomes. A discrete variable is of one of two subtypes: categorical or ordinal. 2.1 Categorical data Categorical variables are discrete variables with no particular ordering of the categories. 2.1.1 Examples – categorical data The classical example of a categorical variable is sex. For each subject, the value of this variable is one of two possible values: male or female. Other examples: color. We generally work with a set number of categories. race blood type country of origin political orientation 2.2 How to describe categorical data Categorical variables are often described using frequency counts and relative frequencies. Frequency counts (or simply frequency) are found simply by counting how many times each possible value of an outcome is present in the data. Relative frequencies are found by dividing the frequency by the total number of observations. 2.2.1 Examples Below are the frequencies for some categorical variables in the SHOW data set. edu: Value Frequency valid_percent 0 2 0.0005926 10 49 0.01452 11 66 0.01956 12 90 0.02667 13 624 0.1849 14 112 0.03319 15 680 0.2015 16 437 0.1295 17 191 0.05659 18 723 0.2142 19 263 0.07793 20 47 0.01393 21 40 0.01185 3 2 0.0005926 4 1 0.0002963 6 4 0.001185 7 2 0.0005926 8 20 0.005926 9 22 0.006519 NA 6 NA gender: Value Frequency valid_percent 0 1479 0.4376 1 1901 0.5624 NA 1 NA marital: Value Frequency valid_percent 1 2075 0.6146 2 113 0.03347 3 416 0.1232 4 41 0.01214 5 603 0.1786 6 126 0.03732 D 2 0.0005924 NA 5 NA race: Value Frequency valid_percent 1 2870 0.8511 2 243 0.07206 3 108 0.03203 4 151 0.04478 NA 9 NA We can add relative frequencies to this simply by dividing each frequency by the total number of observations. edu: Value Frequency Relative Frequency valid_percent 0 2 0.0005915 0.0005926 10 49 0.01449 0.01452 11 66 0.01952 0.01956 12 90 0.02662 0.02667 13 624 0.1846 0.1849 14 112 0.03313 0.03319 15 680 0.2011 0.2015 16 437 0.1293 0.1295 17 191 0.05649 0.05659 18 723 0.2138 0.2142 19 263 0.07779 0.07793 20 47 0.0139 0.01393 21 40 0.01183 0.01185 3 2 0.0005915 0.0005926 4 1 0.0002958 0.0002963 6 4 0.001183 0.001185 7 2 0.0005915 0.0005926 8 20 0.005915 0.005926 9 22 0.006507 0.006519 NA 6 0.001775 NA gender: Value Frequency Relative Frequency valid_percent 0 1479 0.4374 0.4376 1 1901 0.5623 0.5624 NA 1 0.0002958 NA marital: Value Frequency Relative Frequency valid_percent 1 2075 0.6137 0.6146 2 113 0.03342 0.03347 3 416 0.123 0.1232 4 41 0.01213 0.01214 5 603 0.1783 0.1786 6 126 0.03727 0.03732 D 2 0.0005915 0.0005924 NA 5 0.001479 NA race: Value Frequency Relative Frequency valid_percent 1 2870 0.8489 0.8511 2 243 0.07187 0.07206 3 108 0.03194 0.03203 4 151 0.04466 0.04478 NA 9 0.002662 NA Relative frequencies are useful when trying to compare the values of a specific variable across groups. Say we want to investigate if there are any differences in the marital status between genders in this cohort. We could consider the frequency of marital status stratified by gender: ## gender 1 2 ## 0 0.627450980392157 (928) 0.0209601081812035 (31) ## 1 0.603366649132036 (1147) 0.0431351920042083 (82) ## NA 0 (0) 0 (0) ## 3 4 5 ## 0.103448275862069 (153) 0.00540906017579446 (8) 0.204868154158215 (303) ## 0.138348237769595 (263) 0.0173592845870594 (33) 0.157811678064177 (300) ## 0 (0) 0 (0) 0 (0) ## 6 D NA_ ## 0.0358350236646383 (53) 0.000676132521974307 (1) 0.00135226504394861 (2) ## 0.038400841662283 (73) 0.000526038926880589 (1) 0.00105207785376118 (2) ## 0 (0) 0 (0) 1 (1) ## Total ## 1 (1479) ## 1 (1901) ## 1 (1) The table above shows the relative frequencies of marital status within each gender (frequency counts in parentheses). So we can see that the same proportion of… 2.3 Ordinal Data An ordinal variable is a discrete variable where the groups can easily be ordered in a meaningful sense. 2.3.1 Examples 2.4 How to visualize discrete data Categorical data is often best presented using a bar chart of either frequency counts or relative frequencies. 2.4.1 Bar Charts Below is a bar chart of the frequency counts of the marital status varaible from the SHOW data. This can easily be turned into a bar chart of the relative frequencies. "],
["continuous.html", "3 Continuous 3.1 How to describe continuous data 3.2 How to visualize continuous data", " 3 Continuous A continuous variable is a numerical variable that can take on an infinite and uncountable number of possible values. 3.1 How to describe continuous data 3.2 How to visualize continuous data "],
["grey-areas.html", "4 Grey areas", " 4 Grey areas An example of a variable that could easily be mistaken as categorical is age. Often when we think about age, we think about this in terms of years, or months, or even days. In that sense, age is a variable with a number of possible values that we could technically count – start with 0, 1, 2, 3, …, 55, 56, 57, … . However, this is NOT the natural structure of the variable, but rather a limitation of the way it is measured and recorded. Technically, age is the time from birth till now. This manuscript was prepared using the PECARN Core Data Project (PCDP) 2002-2014 (STUDY) Data Set obtained from UTAH, and does not necessarily reflect the opinions or views of the STUDY investigators or the Health Resources Services Administration (HRSA) Maternal Child Health Bureau (MCHB) Emergency Medical Services for Children (EMSC). The PECARN was funded by the HRSA/MCHB/EMSC. "],
["what-is-probability.html", "5 What is “probability”? 5.1 Definitions", " 5 What is “probability”? A probability is a number between 0 and 1 that indicates how likely it is that a certain event happens. An event that has the probability of 1 always occurs, while an event with probability of 0 never occurs. Every number in between are a bit harder to interpret. For example, an event with probability 0.5 supposedly happens every other time. This makes sense if you think about something that can be repeated, such as a coin flip, or the roll of a die, but how does that work if we consider an event that only occurs once? For example, how do we interpret a weather forecast that claims there’s a 0.5 (i.e. 50%) chance of rain tomorrow? We can only observe if it rains tomorrow or not once, so the probability surely must be 0 (it doesn’t rain) or 1 (it rains), right? 5.1 Definitions As hinted at above, the concept of “probability” can be a bit challenging to wrap your head around. There are generally two ways that the term is introduced. Though they are very similar once you understand the concepts, they can seem radically different at first. Definition 5.1 The probability of an event is the number of outcomes that ensure the event happens divided by the total number of possible outcomes, IF all outcomes are equally likely: \\[ P(\\text{event}) = \\frac{\\text{number of outcomes that result in event}}{\\text{total number of possible outcomes}}. \\] We often refer to the numerator in this fraction as the number of favorable outcomes. I want to take a second to draw your attention to that small, but incredibly important, final bit of the definition: “IF all outcomes are equally likely”. We will later discuss what to do if this is not the case, but for now, this will be an underlying assumption. The best way to become comfortable with this definition is by considering a few simple examples. The following two examples are the most commonly used, and by far most boring, examples in the history of statistics. However, they are super useful for two reasons: They are so simple that it is possible to better grasp what’s going on A lot of more complicated examples can be simplified by comparing them to these two Example: coin flip We want to find the probability \\(P(\\text{coin comes up heads})\\). A natural assumption is that when flipping a coin, heads and tails are the only outcomes1, and they are equally likely. Therefore, \\[\\begin{align} P(\\text{coin comes up heads}) &amp;= \\frac{\\text{# possible outcomes that come up heads}}{\\text{# possible outcomes}} \\\\ &amp;= \\frac{1}{2} \\\\ &amp;= 0.5. \\end{align}\\] Similarly, one can find the probability that the coin comes up tails: \\[\\begin{align} P(\\text{coin comes up tails}) &amp;= \\frac{\\text{# possible outcomes that come up tails}}{\\text{# possible outcomes}} \\\\ &amp;= \\frac{1}{2} \\\\ &amp;= 0.5. \\end{align}\\] Example: roll of a die The two examples above show situations where all possible outcomes are equally likely. What if that is not the case? Example: disease status Let us consider the SHOW data set. We might be interested in the probability of a subject being obese. Now, there seems to be only two outcomes here: either the subject is obese, or the subject is not. So, using the same string of thoughts as above, one might conclude that the probability of a subject being obese if \\(\\frac{1}{2}\\), i.e. \\(0.5\\). This is obviously not the case. The problem with this approach is that the two outcomes – those being “the subject is obese”, and “the subject is NOT obese” – are not equally likely, so the simple approach of simply dividing the number of favorable outcomes by the number of possible outcomes is not doing us any good. To find a more satisfying answer to the question asked in the last example, we need to consider a different approach to probabilities. Definition 5.2 The probability of a specific outcome from an experiment is the proportion of times the outcome occurs if the experiment is repeated an infinite number of times. Repeating an experiment an infinite number of times is obviously not possible, so in practice “an infinite number of times” becomes “a very large number of times”. When introducing this different approach to probabilities, first we need to make sure it doesn’t contradict our previous approach. Example: coin flip (revisited) We previously established that when flipping a coin, the probability of heads is \\(0.5\\). Hopefully this new definition will yield a similar answer. To find out if that is actually the case, we would have to flip a coin “an infinite number of times”. Obviously, this is not possible, so we will have to settle for “a very large number of times”. So, imagine we flip a coin 100000 times. Every time it is flipped, we write down the result, and count how many times we’ve seen heads, and how many times we’ve seen tails so far. If the probability of seeing heads is \\(0.5\\), we should eventually see about as many heads as tails. Below is an animation that shows the results of such an experiment. The bars show you the proportion of heads and tails, which in the end (by the definition above) will converge to the probability. The first 100 flips are all shown, then only the results after every 100 flips, and finally results after every 1000 flips are shown. Note how at the very end the two bars are both very close to \\(0.5\\). 5.1.1 Example: roll of a die (revisited) 100000 5.1.2 Example: disease status Okay, so both when flipping a coin and rolling a die, the second definition agrees with the first one. But how can we use this way of thinking in the disease status example? What does it even mean to “repeat the experiment”, let alone “repeat an infinite number of times”?! In such a situation, we make a (very crude, but very necessary) assumption: we assume that all the subjects in the cohort are “similar enough” that we can pretend that observing the disease status of multiple people constitutes multiple experiments. We then estimate the probability of having the disease as the proportion of subjects with the disease. "],
["conditional-probability.html", "6 Conditional Probability Example: roll a die Example: disease status Example: Sensitivity/specificity Example: positive/negative predictive value 6.1 Bayes’ Theorem 6.2 Independence", " 6 Conditional Probability So far, we have talked about probabilities in a context where no additional information is available about the experiment. This is of course not always the case, and also not always what we are interested in. A useful concept in these cases is the concept of conditional probabilities. In a nutshell, conditional probabilities deal with the chances of something happening given something else has already happened. If we consider two events, \\(A\\) and \\(B\\), then we write \\(P(A | B)\\) for the conditional probability of \\(A\\) given that \\(B\\) has happened. Example: roll a die Previously, we considered the probabilities associated with the roll of a die. We found that the probability of rolling a six is \\(\\frac{1}{6}\\). What if we somehow knew that the outcome turned out to be an even number, but simply didn’t know which even number? Well, using this information, we know there are only three possible outcomes, namely \\(2,4,6\\). They are all equally likely, so using the probability of rolling a six given the roll comes up even is \\[\\left .P(\\text{roll a } 6\\ \\right|\\ \\text{roll is even}) = \\frac{1}{3}.\\] Example: disease status Example: Sensitivity/specificity Two important examples of conditional probabilities are the so-called sensitivity and specificity. These are particularly useful when discussing the accuracy of screening tests. The sensitivity of a test is the true positive rate (or fraction). That is, out of the tests performed on individuals with the disease of interest, how many come out positive. I.e. \\(\\text{sensitivity} = P(\\text{test positive}\\ |\\ \\text{individual diseased})\\). Similarly, the specificity of a test is the true negative rate (or fraction), i.e. the proportion of tests performed on healthy individuals that come out negative: \\(\\text{specificity} = P(\\text{test negative}\\ |\\ \\text{individual healthy})\\). It is also often useful to consider the false positive rate (FPR) and false negative rate (FNR). These are defined as follows: \\[\\begin{align*} \\text{FPR} &amp;= P(\\text{test positive}\\ |\\ \\text{individual healthy}), \\\\ \\text{FNR} &amp;= P(\\text{test negative}\\ |\\ \\text{individual diseased}). \\\\ \\end{align*}\\] Let’s consider a concrete example. Below is table 5-5 from Sullivan (2017). This table shows the results of screenings of 4810 pregnant women to assess if their fetus is likely to have Down Syndrome. After birth, it is determined if the child actually has Down Syndrome, provided a ground truth that we can check our screening method against. Ideally, the test is positive for all kids with Down Syndrome, and negative for alld kids without Down Syndrome. Let us calculate the specificity, sensitivity, FNR, and FPR: \\[ \\begin{align*} \\text{specificity} &amp;= P(\\text{test negative}\\ |\\ \\text{child healthy}) \\\\ &amp;= \\frac{\\text{# negative tests among healthy children}}{\\text{# healthy children}} \\\\ &amp;= \\frac{4449}{4800} = 0.927 \\\\ &amp; \\\\ \\text{sensitivity} &amp;= P(\\text{test positive}\\ |\\ \\text{child has Down Syndrome}) \\\\ &amp;= \\frac{\\text{# positive tests among children with Down Syndrome}}{\\text{# children with Down Syndrome}} \\\\ &amp;= \\frac{9}{10} = 0.9 \\\\ &amp; \\\\ \\text{FPR} &amp;= P(\\text{test positive}\\ |\\ \\text{individual healthy}) \\\\ &amp;= \\frac{\\text{# positive tests among healthy children}}{\\text{# healthy children}} \\\\ &amp;= \\frac{351}{4800} = 0.073 \\\\ &amp; \\\\ \\text{FNR} &amp;= P(\\text{test negative}\\ |\\ \\text{individual diseased}) \\\\ &amp;= \\frac{\\text{# negative tests among children with Down Syndrome}}{\\text{# children with Down Syndrome}} \\\\ &amp;= \\frac{1}{10} = 0.1. \\end{align*} \\] We see that the test has some very desirable attributes, in high specificity AND high sensitivity. At this point, some might stop and wonder for a second: the end goal is to determine if the test is accurate, so why don’t we just calculate the accuracy of the test? I.e. what’s wrong at simply looking at the number of corret test results out of the total number of tests? Let’s take a look. \\[ \\begin{align} \\text{test accuracy} &amp;= \\frac{\\text{# correct results}}{\\text{# tests performed}} \\\\ &amp;= \\frac{9 + 4449}{4810} \\\\ &amp;= \\frac{4458}{4810} = 0.927 \\end{align} \\] That’s pretty impressive. The test has an accuracy rate of almost \\(93\\%\\), i.e. almost \\(93\\%\\) of tests yield the correct result. Now, let us consider a different test for the same disease. Tested on the same 4810 women, let’s pretend it yields the following results: Now, the accuracy rate of this test is \\(\\frac{1 + 4449}{4810} = 0.925\\), i.e. almost the same as the first test. That’s, again, really impressive! But upon further investigation, something is off. The sensitivity is way off. Out of 10 children with Down Syndrome, the test only came back positive for 1, which yields a sensitivity of only only \\(0.1\\). In other words, if a fetus actually is a affected, the test only has a \\(10\\%\\) chance of detecting it. That’s not very comforting. This is a common problem with rare diseases. Since by far most individuals will not be diseased, a test that is good at predicting healthy individuals, but awful at predicting diseased individuals, will have a high accuracy, but such a test is not very desirable. Consider this last test for Down Syndrome: no test is performed, and we just always say the fetus is unaffected. Since 4800 out of 4810 fetuses were unaffected, we have an accuracy of \\(\\frac{4800}{4810} = 0.998\\). Pretty impressive accuracy rate, absolutely useless test… Example: positive/negative predictive value The specificity is the answer to the question “what is the probability the test will be correct when the patient is actually healthy?” This is of course a very important thing to know, and if this probability is very low, the test might not be particularly useful. However, a just as important, and sometimes more relevant, measure is the negative predictive value. This relates to the question “what is the probability the patient is actually healthy when the test comes back negative?” Similarly, we can talk about the positive predictive value. Where the sensitivity is the probability that the test is positive if the patient has the disease, the positive predictive value is the probability that a patient has the disease if the test comes back positive. Let us again consider the Down Syndrome data. Since the negative predictive value is the probability a child is healthy given the test was negative, it calculated as the proportion of children with negative tests that actually were healthy. So, \\[ \\begin{align} \\text{Positive Predictive Value} &amp;= P(\\text{child healthy } | \\text{ test negative}) \\\\ &amp;= \\frac{4449}{4450} \\\\ &amp;= 0.999. \\end{align} \\] Similarly, since the negative positive predictive value is the probability a child has Down Syndrome given the test was positive, it is calculated as the proportion of children with positive tests that actually has Down Syndrome. So, \\[ \\begin{align} \\text{Negative Predictive Value} &amp;= P(\\text{child diseased } | \\text{ test positive}) \\\\ &amp;= \\frac{9}{360} \\\\ &amp;= 0.025. \\end{align} \\] 6.1 Bayes’ Theorem We have seen a few examples of some very useful and meaningful quantities that are actually conditional probabilities. We’ve seen how we, in general, calculate these conditional probabilities, but only in a setting where we know everything. The following theorem (i.e. very big and important result) provides a powerful way of finding conditional probabilities, and it also provides a very useful connection between conditional probabilities, and marginal probabilities (i.e. probabilities that are not conditional). Theorem 6.1 (Bayes’ Theorem) Bayes’ Theorem simply states that \\(P(A | B) = \\frac{P(A \\text{ and } B)}{P(B)}\\). Since \\(P(A \\text{ and } B) = P(B \\text{ and } A)\\), this gives us that \\(P(B | A)P(A) = P(A \\text{ and } B)\\), so \\(P(A | B) = \\frac{P(B | A)P(A)}{P(B)}\\). Especially the latter formulation is very powerful, as we shall see in this next example. Example: positive predictive value from sensitivity This allows us to calculate the positive predictive value using the sensitivity of a test, the prevalence of the disease we’re testing for, and how often the test itself is positive (regardless of patient status). 6.2 Independence One of the big ones in statistics in general is the concept of independence. When things are independent, all the math simplifies a great deal, which is the main reason why a lot of the methods we will consider later on are based on the assumption that observations are independent of one another. Loosely speaking, two events are said to be independent if knowledge about one of the events does not provide any information about the other. I.e. if I ask you what the probabilitity of event A happening is before and after I tell you whether event B happened or not, your answers should be the same. Example: independent events Event A: I walk around Madison one day, stop a random stranger, and ask: “are you taller than 6ft?” Event B: I flip a coin, and it comes up tails. Events A and B are independent. The probability that a random person is taller than 6ft is not altered by the fact that a coin flip comes up tails. Example: dependent events Event A: I walk around Madison one day, stop a random stranger, and ask: “are you taller than 6ft?” Event B: The random stranger I stop is male. Events A and B are NOT independent. The probability a random stranger is taller than 6ft is about 0.16 if the person is male, but less than 0.01 if the person is female.2 So the probability of event A being ‘yes’ depends on the outcome of event B. Therefore, they are not independent. We will work with two definitions of independence. (Fortunately, they are equivalent, i.e. if one holds, the other holds.) Definition 6.1 Two events are independent if and only if \\(P(A \\text{ and } B) = P(A) P(B)\\). Definition 6.2 Two events are independent if and only if \\(P(A | B) = P(A)\\) AND \\(P(B | A) = P(B)\\). Example B Simulating Data "],
["random-variables-and-distributions.html", "7 Random Variables and Distributions 7.1 Random Variables 7.2 Properties of Random Variables 7.3 A Few Important Distributions", " 7 Random Variables and Distributions So far in this section, we’ve talked about probabilities, different ways of thinking about probabilities, and a bit about how to work with probabilities. In this section we will introduce a more formal framework for how to think about and handle uncertain events. By making a few assumptions about how things behave, we can calculate probabilities of events without observing them. 7.1 Random Variables A random variable is a variable where the value is not guarenteed in advance, but can take different values. 7.1.1 Examples: random variables Define a variable \\(X\\) to be the outcome of a coin flip. Now, before we flip the coin, we do not know what value \\(X\\) will take on – it could be “heads” or it could be “tails”. Once we flip the coin and observe the outcome, we say that we have a realization of the random variable \\(X\\). Another example would be if we let \\(Y\\) be the height of a randomly chosen US adult. We don’t know exactly what value it is, but we do know a few things about it. For example, it is much more likely to be around 5.5ft than it is to be around 7ft or below 4ft. When we do finally randomly select a US adult, and measure their height, we get a realization of this random variable. A third example is if we let \\(Z\\) denote the diabetes status of a randomly chosen US adult. This could take the values healthy, type I, or type II, and each will happen with some probability. As the example above was meant to illustrate, a random variable can really be anything you’d like. And whenever we talk about a random variable, we also talk about the probability of certain outcomes. If we can define a way to calculate probabilities of different outcomes of the random variable, we call this the distribution of the random variable. Recall previously we talked about two kinds of variables: discrete and continuous variables. Likewise, we can consider both discrete and continuous random variables. Depending on the kind of random variable we’re discussing, defining it’s distribution is handled slightly differently. When we consider discrete random variables, its distribution is defined by specifying the probability of every single possible outcome. There are two things that are important to remember: all probabilities must be between \\(0\\) and \\(1\\), the sum of all probabilities must add up to \\(1\\). The second point above is important, and is sometimes super handy when trying to calculate probabilities of certain complicated events. The intuition behind it is pretty simple: something must happen. So the probability that something happens is \\(1\\). 7.1.2 Examples: discrete distributions Consider \\(X\\) the outcome of a coin flip. The outcome of this can be one of two things: heads or tails. Now, let us pretend that this particular coin is NOT fair, i.e. it is not 50/50. Maybe the probability of getting tails is 0.4. Maybe the probability of getting heads is 0.1. For now, let the probability of getting heads be \\(p\\), some number between 0 and 1. Then the probability of \\(X\\) coming up as heads is \\(p\\), and the probability it comes up as tails is \\(1-p\\), since the sum of all probabilities has to be \\(1\\). We write \\(P(X = \\text{heads}) = p\\) and \\(P(X = \\text{tails}) = 1-p\\). This is the distribution of \\(X\\). In this case, all we need is the probability of the two outcomes. Another example: let \\(X\\) be the marital status of a randomly chosen participant from the SHOW data. The examples above all consider discrete random variables. As already mentioned, the approach for continuous random variables is a bit different. For the distribution of a continuous random variable, we need to specify a curve for which the area under the curve is \\(1\\). When we talk about probabilities of events that relate to the correpsonding random variable, we talk about areas under the curve. 7.1.3 Examples: continuous distributions 7.2 Properties of Random Variables When we talk about random variables, there is a great deal of uncertainty involved, since (by design) we do not know exactly what values the random variables will take after a conducted experiment. Similarly, we cannot be sure that repeating an experiment results in the same outcomes of the random variables simply since they are, as the name strongly implies, random. However, if we have some information about the random variable we’re interested in, we can talk about some very important features of the random variable. The two we will talk about here are the expected value and variance/standard deviation of random variables. These two concepts can be a bit hard to wrap ones head around at first, but as we talk about them over and over agian, hopefully you will realize that they are not as abstract as they might first seem. 7.2.1 Expected Value of Random Variables The expected value of a random variable is, intuitively, the long run average. I.e. if we repeat an experiment an infinite number of times, we can determine the expected value of a random variable as the average of all the realizations of said random variable. As an example, if we consider the random variable \\(X\\) that is \\(0\\) if a coin flip comes up heads, and \\(1\\) if it comes up tails, we can imagine flipping a coin an infinite number of times, and calculating the average. The result would be that the expected value of \\(X\\) is \\(0.5\\). We write \\(E(X) = 0.5\\). Since the expected value can be thought of as the long run average, it is in some sense the value that the outcomes of the random variable are going to be centered around. Note: the expected value is also often referred to as the mean value. For any discrete random variable where we know the distribution, we can find the expected value in the following way: \\(E(X) = x_1 \\cdot P(X = x_1) + ... + x_n P(X = x_n) = \\sum_{i=1}^n x_i P(X = x_i)\\).3 7.2.1.1 Example: expected value of discrete random variable Let \\(X\\) be a discrete random variable the can take the values \\(1,2,6\\), and \\(12\\). Let the probabilities of each outcome be as follows: x P(X = x) 1 0.2 2 0.1 6 0.6 12 0.1 Then we can calculate the expected value of \\(X\\): \\[ \\begin{align} E(X) &amp;= \\sum_{i = 1}^4 x_i P(X = x_i) \\\\ &amp;= 1 \\cdot P(X = 1) + 2 \\cdot P(X = 2) + 6 \\cdot P(X = 6) + 12 \\cdot P(X = 12) \\\\ &amp;= 1\\cdot 0.2 + 2\\cdot 0.1 + 6 \\cdot 0.6 + 12 \\cdot 0.1 \\\\ &amp;= 5.2. \\end{align} \\] So what does this mean? It means that if we perform an experiment that results in a realization of the random variable \\(X\\) many, many, many times, the average of all outcomes is going to be close to \\(5.2\\). 7.2.1.2 Example: expected value of a continuous random variable In the continuous case, actually calculating the expected value isn’t as easy as in the discrete case. Remember, when we specify a discrete distribution, we specify the probability of each possible outcome. When we specify a continuous distribution, we specify a curve over all the possible outcomes, and probabilities of specific events correspond to areas under the curve. This also means that it is impossible to use a formula like the one introduced for the discrete case above. Fortunately, the intuition is the same. The expected it the long run average. 7.2.1.3 Rules for working with expected values Sometimes, it is very beneficial to be able to transform a random variable, or combine several random variables, into a new one, and work with that new random variable. Fortunately, dealing with the expected value of a large number of such transformations is pretty simple. First, let’s imagine we have a random variable \\(X\\) with mean \\(E(X)\\), and another random variable \\(Y\\) with mean \\(E(Y)\\). Perhaps we are interested in the sum of the two, so we construct a new random variable \\(Z = X + Y\\).4 Finding the expected value of \\(Z\\) is really simple: \\(E(Z) = E(X + Y) = E(X) + E(Y)\\). In words: the expected value of a sum of random variables is simply the sum of expected values. Another example: maybe we want to scale the outcome of the random variable \\(X\\) by a constant \\(a\\), and then consider the new random variable \\(Y = a\\cdot X\\).5 Again, finding the expected value of the new random variable \\(Y\\) is really simple: \\(E(Y) = E(aX) = a\\cdot E(X)\\). One final thing I want to mention here: the expected value of a constant will always be the constant itself. Hopefully, this doesn’t come as too much of a shock. The expected value is what we would expect from a random variable. If something is constant, it means it never changes, so we expect it to stay the same. So, if \\(a\\) is a constant, \\(E(a) = a\\). This can be combined with the first rule we talked about to give us that \\(E(X + a) = E(X) + E(a) = E(X) + a\\). 7.2.2 Variance/Standard Deviation of Random Variables Where the expected value of a random variable tells us something about where the outcomes of the random variable tend to be located, the next measures we’ll be looking at tell us something about how spread out the outcomes will be around the expected value. Note: most textbooks handle the variance and standard deviations as two distinct things. I don’t like that. They are virtually two sides of the same coin, and I will deliberately handle the two at the same time. My reasoning for this is that, at least in my head, these two measures try to convey the same message, but to two different audiences. I will elaborate on this later, but try to keep in mind that these two measures are almost the same. The variance of a random variable is a measure that tells us how much we expect the outcome of said random variable to vary from the expected value. As with the expected value, it is relatively simple to calculate this when we are dealing with simple discrete random variables. Let \\(X\\) be a discrete random variable with possible outcomes \\(x_1, ..., x_n\\), and the probability of \\(x_i\\) is \\(P(X = x_i)\\). Then the variance of \\(X\\) is \\(\\text{Var}(X) = \\sum_{i=1}^n P(X = x_i)(x_i - E(X))^2\\). At first glance, this can look a bit intimidating, so let’s try to break it down to better understand what’s going on: It actually has the form of an expected value, i.e. it is a sum of where each term is the product of the value of an outcome and the probability of that outcome. So, intuitively, this is not much different than an expected value, it’s just an expected value of something else. That “something else” is \\((x_i - E(X))^2\\). This is representative of the distance from the outcome \\(x_i\\) to the expected value… … except, we square the distance. We do this because we want this measure to be representative of the variation of the data, and so we cannot allow positive and negative differences to cancel. Example: if we didn’t square the differences, a random variable with possible outcomes \\(1,2,3\\) each with probability \\(1/3\\) would have variance \\(0\\), but clearly there is some variation in the sample – not all observations are the same. So, loosely speaking, the variance is “a measure of averaged distances from observations to the sample average”. 7.2.2.1 Rules for working with variance Working with the variance of random variables is not quite as simple as working with the expected value. This is due to the fact that the expected value is a simple average, whereas the variance is an average of squared differences. The result is the following set of rules: if \\(X\\) and \\(Y\\) are random variables, and \\(a\\) is some fixed constant, then \\(\\text{Var}(a\\cdot X) = a^2 \\text{Var}(X)\\), \\(\\text{Var}(a) = 0\\), if \\(X\\) and \\(Y\\) are independent: \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\). Combining (1) and (2) above tells us that, if \\(X\\) and \\(Y\\) are independent, then \\(\\text{Var}(X - Y) = \\text{Var}(X + (-Y)) = \\text{Var}(X) + \\text{Var}(-Y) = \\text{Var}(X) + (-1)^2 \\text{Var}(Y) = \\text{Var}(X) + \\text{Var}(Y)\\). Don’t forget this!! 7.2.2.2 So what about that standard deviation? So far we’ve talked about the variance, a bit about how to interpret it, and how to work with it for multiple random variables. But what about that other thing mentioned above, the standard deviation? The standard deviation of a random variable is simply the square root of the variance: \\(\\text{SD}(X) = \\sqrt{\\text{Var}(X)}\\). As we saw above, the variance has some nice mathematical properties, such as the fact that it is basically an expected value, and that (when \\(X\\) and \\(Y\\) are independent) \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\). Neither of these two things are true for the standard deviation, and we lose both because of the square root. However, it is also because of an effect of the square root that we like using the standard deviation in certain situation. As mentioned, the variance is nice mathematically, but as soon as we make our way back from the beautiful haven that is the Land of Mathematics, and want to communicate our findings to collaborators or the rest of the world, the variance isn’t great. Since we square all the differences, the unit of the variance is whatever unit your original measure was squared. Example: we might wish to estimate the height of adults in the SHOW data, and report it with some measure of uncertainty. We find that the average height is NA inches, and the variance is NA… \\(\\text{inches}^2\\)? This is hard to really grasp, and the number itself doesn’t mean much to us. Is 22 \\(\\text{inches}^2\\) a lot? We can’t even really compare it to the mean because of the different units! However, the standard deviation fixes just that. It is still a measure of the expected variation, but this has been brought by to the original scale by taking the square root. So when we report a mean height of NA inches with a standard deviation of NA inches, this all of a sudden makes much more sense intuitively. The moral of the story: both the variance and the standard deviation have a role in the world of statistics, but at different stages. The variance is very useful in the more mathematical parts of the field, while the standard deviation is easier to interpret. Luckily, going from one to the other is simple: \\(\\text{Var}(X) = \\text{SD}(X)^2\\) and \\(\\text{SD}(X) = \\sqrt{\\text{Var}(X)}\\). Therefore, if you ever have one, you practically have both. Don’t forget this, as it is a common mistake to plug in the variance to equations where it should have been the standard deviation. 7.2.3 Things to remember when working with random variables When working with random variables, \\(X\\) and \\(Y\\), these are the important rules: \\(E(X + Y) = E(X) + E(Y)\\), if \\(a\\) is some fixed number, \\(E(a\\cdot X) = a\\cdot E(X)\\), if \\(a\\) is some fixed number, \\(\\text{Var}(a \\cdot X) = a^2 \\text{Var}(X)\\), IF \\(X\\) and \\(Y\\) are independent, \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\), IF \\(X\\) and \\(Y\\) are independent, \\(\\text{Var}(X - Y) = \\text{Var}(X) + \\text{Var}(Y)\\). Things people often forget: \\(E(X\\cdot Y) \\neq E(X)E(Y)\\), \\(E\\left(\\frac{X}{Y}\\right) \\neq \\frac{E(X)}{E(Y)}\\), \\(\\text{Var}(X+Y) \\neq \\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are not independent, \\(\\text{Var}(X - Y) \\neq \\text{Var}(X) - \\text{Var}(Y)\\), \\(\\text{SD}(X + Y) \\neq \\text{SD}(X) + \\text{SD}(Y)\\), even when \\(X\\) and \\(Y\\) are independent. 7.3 A Few Important Distributions 7.3.1 The Bernoulli Distribution In the first example above, we consider flipping a coin. Such an experiment, i.e. one with only two possible outcomes, is often referred to as a Bernoulli experiment, and the random variable \\(X\\) is referred to as a Bernoulli random variable. The “probability of success” (you get to pick our favorite outcome as a success) is often denoted \\(p\\). As a shorthand for such a random variable, we write \\(X \\sim \\text{Bernoulli}(p)\\), which is read as “\\(X\\) follows a Bernoulli distribution with probability parameter \\(p\\)” or “\\(X\\) is Bernoulli distributed with parameter \\(p\\)”. Phrases like these can sometimes sound scary and complex, but all it means is that the random variable \\(X\\) can only take on two different outcomes, and the probability of \\(X\\) being one of the two outcomes is \\(p\\), the probability of it being the other is \\(1-p\\). (Important note: remember that the sum of all probabilities has to be \\(1\\), so if the probability of one outcome is \\(p\\), and there are only two possible outcomes, then the probability of the other outcome must be \\(1-p\\). This way of thinking is something we will use over and over again.) Using the properties discussed in section 7.2, we can calculate the expected value and variance of a Bernoulli random variable. Simply using the definitions, we see that \\[ E(X) = \\sum_{i=1}^2 x_i \\cdot P(X = x_i) = 0 \\cdot P(X = 0) + 1 \\cdot P(X = 1) = p, \\] and \\[ \\begin{align} \\text{Var}(X) &amp;= \\sum_{i=1}^2 P(X = x_i) \\cdot (x_i - E(X))^2 \\\\ &amp;= P(X = 0)\\cdot (0 - p)^2 + P(X = 1)\\cdot (1 - p)^2 \\\\ &amp;= (1 - p)\\cdot p^2 + p\\cdot (1 - p)^2 \\\\ &amp;= (1-p)\\cdot(p^2 + p\\cdot(1-p)) \\\\ &amp;= (1-p)\\cdot(p^2 + p - p^2) \\\\ &amp;= (1-p)\\cdot p. \\end{align} \\] So it is actually rather simple to find the expected value and variance of a Bernoulli random variable, if we know the probability of success (\\(p\\)). 7.3.2 The Binomial Distribution Often times we are interested in things that can be viewed as a sum of Bernoulli random variables. Let’s say we have \\(n\\) independent (i.e. the outcome of one doesn’t say anything about the rest) Bernoulli random variables (\\(X_1\\), \\(X_2\\), …, \\(X_n\\)), all with probability of success \\(p\\), and are interested in the sum of those \\(n\\) variables \\(Y = X_1 + X_2 + ... + X_n\\). For this to make sense, we let \\(X_i\\) be \\(1\\) if the corresponding “experiment” is a success, and \\(0\\) if it is a failure. Now, we can think of the random variable \\(Y\\) as either (1) the sum of independent Bernoulli random variables, or (2) the number of successes among \\(n\\) independent trials with binary outcomes. It is this latter interpretation that makes the random variable \\(Y\\) interesting. When a random variable is the sum of \\(n\\) independent Bernoulli random variables all with probability of success \\(p\\), we say that \\(Y\\) follows a Binomial distribution with size \\(n\\) and probability of success \\(p\\). We write \\(Y \\sim \\text{Binomial}(n,p)\\). Let’s think for a second about what possible values \\(Y\\) can take. If all \\(n\\) Bernoulli experiments happen to come out as failures, then all \\(X_i\\)’s are \\(0\\)’s, and so \\(Y\\) will also be \\(0\\). The other extreme is if all \\(n\\) Bernoulli experiments are successes, then all \\(X_i\\)’s are \\(1\\)’s, and \\(Y\\) will be the sum of \\(n\\) \\(1\\)’s, so \\(Y\\) will be \\(n\\). These are simply the two extremes - any number of the \\(X_i\\)’s can be \\(1\\)’s, so \\(Y\\) can end up being any integer between \\(0\\) and \\(n\\), both included. The most likely scenarios are the integers closest to the middle. Since \\(Y\\) is simply a sum of very simple random variables, namely Bernoulli random variables, we can with very simple tools dive deeper, and try to explore what the distribution of a Binomial random variable looks like. We can find the expected value and variance, and the probability of all possible outcomes. There are two ways of doing this: (1) do the math, or (2) flip \\(n\\) coins an infinite number of times and see how often the number of heads is each of the possible outcomes. Let’s start with the latter. Since it’s impossible to flip \\(n\\) coins (for what is \\(n\\)?), we have to pick a real integer. Let’s pick \\(10\\). Similarly, it’s impossible to flip \\(10\\) coins an infinite number of times, so let’s just do it a bunch of times (i.e. \\(`r `\\)). What we are about to do is repeat an experiment (flip \\(10\\) coins) many, many (\\(50000\\)) times. The first time we perform this experiment, we see T,T,H,H,T,H,H,T,T,H. When we translate this to \\(0\\) and \\(1\\), it looks like 0,0,1,1,0,1,1,0,0,1. So, the value of the binomial variable \\(Y\\) is 5, since this is the number of heads. Rinse and repeat. The results of all \\(10\\) experiments are shown in the table below. Now we can get a pretty good estimate of the distribution of \\(Y\\). Recall, the distribution of a random variable is simply the probabilities of each possible outcome. The probability of a particular outcome, say \\(Y = 2\\), is the long run proportion of experiments that result in that outcome. So, \\(P(Y = 2) = \\frac{\\text{# experiments with } Y = 2}{\\text{# experiments}} = \\frac{2164}{5\\times 10^{4}} = 0.04328\\). If we do this for every possible value of \\(Y\\), we get something that looks like the following: y # experiments with Y = y Estimated Probability 0 55 0.0011 1 475 0.0095 2 2164 0.04328 3 5882 0.1176 4 10267 0.2053 5 12297 0.2459 6 10234 0.2047 7 5970 0.1194 8 2149 0.04298 9 465 0.0093 10 42 0.00084 We see that the most probable outcomes are around the middle (4,5,6) with proportions above 0.20. Another popular way of displaying this is using a histogram: When viewing this, the probability of a given outcome can be interpreted as the area of the corresponding bar divided by the total area. As mentioned earlier, the distribution of a binomial random variable can also be calculated mathematically. We won’t go into the details here, but I will leave you with the formulat: \\(P(Y = k) = {n \\choose k}p^k (1-p)^{n-k}\\)6. Take a look at the calculated probabilities below, and compare them to the estimates we got by flipping \\(10\\) coins \\(50000\\) times. y # experiments with Y = y Estimated Probability Probability 0 55 0.0011 0.0009766 1 475 0.0095 0.009766 2 2164 0.04328 0.04395 3 5882 0.1176 0.1172 4 10267 0.2053 0.2051 5 12297 0.2459 0.2461 6 10234 0.2047 0.2051 7 5970 0.1194 0.1172 8 2149 0.04298 0.04395 9 465 0.0093 0.009766 10 42 0.00084 0.0009766 Pretty close! As mentioned, the expected value is basically the long run average. So, if we calculate the average of all outcomes of \\(Y\\) we get a good estimate of what the expected value of \\(Y\\) is. Similarly, the variance of the outcomes is a good estimate of the variance of the random variable \\(Y\\). From the data, \\(\\bar{y} = 4.99986\\) and \\(s_Y^2 = 2.4838697\\). Remember those two numbers. If we use the rules of expectation and variance from the previous section, we can find the exact expected value and variance of a binomial random variable with size \\(n\\) and probability of success \\(p\\). Since \\(Y \\sim \\text{Binomial}(n,p)\\) if \\(Y = X_1 + ... + X_n\\), where \\(X_i \\sim \\text{Bernoulli}(p)\\) and all \\(X_i\\)’s are independent, we have that \\[ \\begin{align} E(Y) &amp;= E(X_1 + ... + X_n) &amp;&amp; \\\\ &amp;= E(X_1) + ... + E(X_n) &amp;&amp; \\\\ &amp;= p + ... p &amp;&amp; (E(X_i) = p \\text{ since } X_i \\sim \\text{Bernoulli}(p)) \\\\ &amp;= n\\cdot p, &amp;&amp; \\end{align} \\] and \\[ \\begin{align} \\text{Var}(Y) &amp;= \\text{Var}(X_1 + ... + X_n) &amp;&amp; \\\\ &amp;= \\text{Var}(X_1) + ... + \\text{Var}(X_n) &amp;&amp; (\\text{since all } X_i&#39;s \\text{ are independent}) \\\\ &amp;= p\\cdot(1-p) + ... + p\\cdot(1-p) &amp;&amp; (\\text{since } X_i \\sim \\text{Bernoulli}(p)) \\\\ &amp;= n\\cdot p \\cdot (1-p). &amp;&amp; \\end{align} \\] These two equations really emphasize that a Binomial random variable is really just \\(n\\) Bernoulli’s: notice how both the expected value and the variance is \\(n\\) times that of a single Bernoulli random variable. Now let’s calculate the expected value and variance of our little experiment. We flip a coin \\(10\\) times. The probability of success is \\(0.5\\). So, \\(Y \\sim \\text{Binomial}(10, 0.5)\\), and we should have \\(E(Y) = 10 \\cdot 0.5 = 5\\), and \\(\\text{Var}(Y) = 10 \\cdot 0.5 \\cdot (1-0.5) = 2.5\\). Remember what we got for the expected value and variance? Numbers very close to these. 7.3.3 Normal Distribution The normal distribution is most definitely the most important distribution we will discuss in this class for one reason: The Central Limit Theorem. We’ll get back to what this is later, but first let us get familiar with the normal distribution. In contrast to the Bernoulli and Binomial distributions, the normal distribution is a continuous distribution. This means that we can not specify the probability of every single possible outcome. Instead we simply specify it using a curve. We then about areas under this curve as the probabilities. This curve is what we call the density. The normal distribution density is specified by two parameters7. The first specifies the mean of the distribution (and is therefore called the mean or location paramater). We often use \\(\\mu\\) to denote the mean of a normal distribution, or \\(\\mu_X\\) if we want to really stress that we are talking about the mean of the random variable \\(X\\)8. The second parameter specifies the variance of the distribution. We often use \\(\\sigma^2\\) to denote this, or \\(\\sigma_X^2\\). The mean parameter can really be any real number, while the variance has to be positive. If \\(X\\) follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we write \\(X \\sim N(\\mu, \\sigma^2)\\). So what does this curve actually look like? It’s a bell curve that is centered at the mean \\(\\mu\\) and the shape/width is controlled by the variance \\(\\sigma^2\\). Below are a few examples. The first figure shows varying means, the second varying variances. As the names of the parameters suggest, the actual expected value and variance of a random variable that is normally distributed, \\(X \\sim N(\\mu, \\sigma^2)\\), is simply \\(\\mu\\) and \\(\\sigma^2\\), respectively. 7.3.3.1 Linear Combination of Normal with Constant One really neat property of the normal distribution is that if you add a constant number, \\(a\\), to a random variable you again get something that is normally distributed. Similarly, if you multiply by a constant you get back something that is still normally distributed. The exact normal distribution can easily be specified (recall: to specify a normal distribution, we need to find the mean and variance). For completeness, let’s do this. If \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Y_1 = X+a\\) and \\(Y_2 = a\\cdot X\\) are also normally distributed. Using the properties of expected value and variance from section 7.2, we get that \\[\\begin{aligned} E(Y_1) &amp;= E(X+a) = E(X) + a = \\mu + a, \\\\ E(Y_2) &amp;= E(a\\cdot X) = a E(X) = a\\cdot \\mu, \\end{aligned}\\] and \\[ \\begin{aligned} \\text{Var}(Y_1) &amp;= \\text{Var}(X+a) = \\text{Var}(X) = \\sigma^2, \\\\ \\text{Var}(Y_2) &amp;= \\text{Var}(a\\cdot X) = a^2 \\text{Var}(X) = a^2 \\sigma^2. \\end{aligned} \\] So \\(X + a \\sim N(\\mu + a, \\sigma^2)\\), and \\(a\\cdot X \\sim N(a\\cdot \\mu, a^2 \\sigma^2)\\). One particular case of the normal distribution plays an important role in much of statistics, and is therefore been named the Standard Normal Distribution. For historic reasons, we often use \\(Z\\) to denote the standard normal distribution, which is simply a normal distribution with mean \\(0\\) and variance \\(1\\). I.e. \\(Z \\sim N(0,1)\\). One reason why this is important is that it provides sort of a baseline that we can always revert to. Whenever you are working with a normal distribution, you can use the rules above to get the standard normal. If \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(\\frac{X-\\mu}{\\sigma} = Z \\sim N(0,1)\\). Why? As we just discussed, adding a constant to a normal random variable results in something normal. \\(X-\\mu\\) is simply adding \\(-\\mu\\) to \\(X\\), so this is still normal. We also saw that multiplying by a constant is still normal, so since \\(\\frac{X-\\mu}{\\sigma}\\) is simply multiplying \\(X\\) by \\(\\frac{1}{\\sigma}\\), we have that \\(\\frac{X-\\mu}{\\sigma}\\) is a normal random variable. We can find it’s mean and variance using the rules we’ve learned, and get that \\(E\\left(\\frac{X - \\mu}{\\sigma}\\right) = \\frac{E(X) - \\mu}{\\sigma} = 0\\), and \\(\\text{Var}\\left(\\frac{X - \\mu}{\\sigma}\\right) = \\frac{\\text{Var}(X)}{\\sigma^2} = 1\\), so \\(\\frac{X-\\mu}{\\sigma} = Z \\sim N(0,1)\\). 7.3.3.2 Sum of (Independent) Normals Another really important and useful property of the normal distribution is that if you have two normally distributed variables, \\(X \\sim N(\\mu_X, \\sigma_X^2)\\) and \\(Y \\sim N(\\mu_Y, \\sigma_Y^2)\\), then the sum of the two, \\(X + Y\\), is also a normally distributed random variable. The mean parameter of this newly created random variable is always easy to find: \\(E(X + Y) = E(X) + E(Y) = \\mu_X + \\mu_Y\\). The variance is, in general a bit harder, except if the two are independent of each other. In this case \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) = \\sigma_X^2 + \\sigma_Y^2\\). So, if \\(X\\) and \\(Y\\) are independent, then \\(X + Y \\sim N(\\mu_X + \\mu_Y, \\sigma_X^2 + \\sigma_Y^2)\\). Combining this with the rules stated above, we get that \\(X - Y\\) is normally distributed as well, since \\(X - Y = X + (-Y)\\), and both \\(X\\) and \\(-Y\\) are normally distributed. More applications of the rules give us \\(X - Y \\sim N(\\mu_X - \\mu_Y, \\sigma_X^2 + \\sigma_Y^2)\\). (NOTE: we DO NOT subtract the variances.) 7.3.3.3 Some Exploration through Simulations To illustrate the properties presented in the two previous sections, let us take a look at some simulated data. Let \\(X \\sim N(-0.5, 1)\\) and \\(Y \\sim N(1, 1.5)\\). I.e. \\(X\\) and \\(Y\\) follow these two distributions: Now, let’s say we’re actually interested in \\(W = X - Y\\). That is, we perform an experiment, observe a realization of \\(X\\) and \\(Y\\), and then create a realization of \\(W\\) as \\(w = x - y\\). The first experiment results in \\(x = 1.5653, y = 0.7306\\), and so \\(w = x - y = 0.8347\\). We repeat this experiment many, many (\\(10^{4}\\)) times. This enables us to take a look at histograms of the outcomes of \\(X\\), \\(Y\\), and \\(W\\), and we calculate the observed averages and variances so that we can compare with our theoretical expectations. So, first of all: do \\(X\\) and \\(Y\\) actually match the distributions we wanted them to come from? Below are histograms of the outcomes with the distributions overlayed. Notice how closely the histograms follow the curves. It definitely seems that the outcomes of \\(X\\) and \\(Y\\) indeed come from the respective normal distributions. Now, let us take a look at the difference between the two, i.e. \\(W\\). A few things to notice: it most definitely looks like a new normal distribution it seems to be centered not far from \\(-1.5\\) it seems to be wider than both of the other curves So, do these observations match what we would expect? We know that the difference of two normally distributed variables should again be normally distributed Our rules tell us that \\(E(W) = E(X - Y) = E(X) - E(Y) = -0.5 - 1 = -1.5\\), so that also checks out The rules stated above also tell us that \\(\\text{Var}(W) = \\text{Var}(X - Y) = \\text{Var}(X) + \\text{Var}(Y) = 1 + 1.5 = 2.5\\), so we do expect the new curve to be wider than both of the old ones. Finally, we can check that the averages and variances we observe are close to what the theory tells us: Variable Average Observed Variance Mean Variance X -0.5061716 0.9892788 -0.5 1.0 Y 0.9963078 1.5060952 1.0 1.5 W -1.5024794 2.4628971 -1.5 2.5 Again, only small differences between the observed and the expected. 7.3.4 t-distribution The t-distribution is very similar to the normal distribution in that the curve also resembles a bell. Unlike the normal distribution, it only depends on one parameter, which is called the degrees of freedom, or \\(df\\). We use the notation \\(t_{df}\\) for a t-distribution with \\(df\\) degrees of freedom. The t-distribution is always centered around \\(0\\), which is also its mean, but the variance depends on the degrees of freedom: if \\(X \\sim t_{df}\\), then \\(\\text{Var}{X} = \\frac{df}{df-2}\\) if \\(df &gt; 2\\), \\(\\text{Var}{X} = \\inftu\\) if \\(1 &lt; df &lt; 2\\), and the variance of \\(X\\) is actually undefined if \\(df &lt; 1\\). Below are a few examples of the t-distribution with different degrees of freedom. For comparison, the standard normal is also included. Notice how similar the t-distributions with more than 9 degrees of freedom look, and how they keep getting closer and closer to the standard normal distribution. It can actually be shown that if we had an infinite number of degrees of freedom, then the t-distribution is identical to the standard normal distribution. 7.3.5 Other Distribution The four distributions above are the ones we’ll consider, but there are many, many more out there. Here are a few examples. 7.3.5.1 Poisson Distribution The Poisson distribution is often used for counting things, such as the number of patients showing up in a clinic during a specified time period. It is a discrete distribution that only returns integer values. It depends on only one parameter which is often referred to as the rate parameter. It is displayed below with a few different values of the rate. For a Poisson distributed random variable with rate parameter \\(\\lambda\\), \\(X \\sim \\text{Poisson}(\\lambda)\\), it holds that \\(E(X) = \\text{Var}(X) = \\lambda\\). 7.3.5.2 Exponential Distribution The exponential distribution is often used for wait times. This can be useful if you want to model the wait times in an emergency room, for example. It is a continuous distribution that depends on a single parameter, which is also called the rate parameter. For a random variable that is exponentially distributed with rate parameter \\(\\lambda\\), \\(X \\sim \\text{Exp}(\\lambda)\\), it holds that \\(E(X) = \\frac{1}{\\lambda}\\), and \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\). "],
["estimators-and-their-distributions.html", "8 Estimators and their distributions 8.1 What is an Estimator? 8.2 Common Estimators 8.3 Deriving Distributions in Practice", " 8 Estimators and their distributions In Part I, we discussed how we can describe and summarize collected data. Different research questions lead you to collect different types of data, and depending on the type of data, there are different ways to present it. So far in this part, we’ve talked about this super abstract concepts, such as probability random variables and distributions that at have nothing to do with the real world. So why did we spent so much time talking about these things? In this section, we will see how random variables and distributions can help us answer questions about the data we collect in the real world. With a few assumptions we will be able to talk about probabilities of real world events, and later on we will use these probabilities to answer questions such as “is it likely that the mean heights of adult men and women in the US are the same?” 8.1 What is an Estimator? Recall the setup: on one hand, we have a population that we are interested in. In this population, there’s some feature that we would like to learn more about. This could be either a continuous measurement (such as height, blood pressure, glucose level, etc), or discrete (marital status, disease status, etc). If we could go out and simply inspect every individual in this population, we could learn the truth. We could find out exactly what proportion of the population have a certain disease, what is the mean glucose level among non-diabetics, and so on. Unfortunately, this is not feasible. What we do instead is we get a sample of individuals from the population. We do this in a way that ensures that this sample is representative of the population, meaning things we might observe in the sample are close to what we would observe in the population, if we had the chance. After collecting a representative sample, we think for a second about exactly what parameter of the population is of interest to us, and then we pick “something” we can actually calculate based on our sample that is close to the parameter of interest. This “something” is what we call the estimator – it is our best guess of what the parameter is based on a sample. An important thing to realize here is that an estimator is a random varaible. The specific value of it depends on the sample we get, which by nature is random. Therefore, repeating the experiment leads to a different value of the estimator. The hope is that the estimator doesn’t vary too much when repeating the experiment, and that the estimator is actually close to the true value of the population parameter. Since an estimator is a random variable, we can talk about the distribution of an estimator. This plays a crucial role when creating confidence intervals and testing hypotheses, as we will see later on in the course. To find the distribution of an estimator, one can take two routes: perform the experiment over and over and over and over again, each time calculating the observed value of the estimator, then drawing a histogram, which in the end will give you the distribution of the estimator, make some assumptions, do some math. The first strategy, as stated here, is not super useful – we can’t possibly afford to repeat every single experiment we do enough times to get enough observed values of the estimator to actually draw a histogram that provides any insights. However, we can do this for made up data, and it turns out that we can tweak this strategy a tiny bit to make it not only useful in practice, but super powerful. The second strategy, although it sounds scary and really hard, turns out to be very useful in a large handful of settings using nothing more complicated than the rules we derived in section 7.2 and THE coolest theorem we see in this class, namely the Central Limit Theorem. The rest of this section will proceed as follows: first, we’ll see a few examples of common estimators. Then, we will explore the distributions of those estimators through simulations and using the SHOW data set as our population. Then we will briefly discuss how to adjust strategy 1 above to make it useful in a practical setting, and finally we will take a look at the Central Limit Theorem, and how we can apply that to back up the distributions we found for different estimators through simulations. 8.2 Common Estimators Some things we are often interested in and their estimators: Parameter of Interest (most commonly used symbol) Estimator Name Notation and Formula Mean of a feature (\\(\\mu\\)) Sample average \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\) Variance of a feature (\\(\\sigma^2\\)) Sample variance \\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2\\) Standard deviation (\\(\\sigma\\)) Sample standard deviation \\(S = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2}\\) Probability of random individual having a disease (\\(\\pi\\)) Proportion in sample with disease \\(P = \\frac{1}{n}\\sum_{i=1}^n X_i\\) Proportion of individuals with disease (\\(\\pi\\)) Proportion in sample with disease \\(P = \\frac{1}{n}\\sum_{i=1}^n X_i\\) As you can see in the table above, most of the estimators we will consider here are pretty much what you would expect. If you are interested in the mean of the population, you look at the average (or mean) of the sample. Interested in the proportion of individuals with a disease in the population? Consider the proportion with that disease in your sample. 8.2.1 Examples In the following examples, we’ll play a game of pretend: pretend that the SHOW cohort is the entire population, and that we would like to estimate different things in this population. Estimating Mean Height Say I ask you to estimate the mean height of the subjects in the SHOW population. I won’t show you the entire population, but I will let you pick a simple random sample of size 20 from the population. You do just that, and you get the following sample. id height hip waste weight race marital gender edu depression_score depression_severity bmi obesity depression_severity_binary 2742 186 102 99 91 1 1 0 13 0 1 26.30362 FALSE 0 472 155 118 103 85 1 5 1 19 11 3 35.37981 TRUE 1 2136 164 91 73 54 1 1 1 19 0 1 20.07733 FALSE 0 1828 176 105 101 91 1 1 0 18 0 1 29.37758 FALSE 0 1673 168 156 135 135 1 3 1 18 2 1 47.83163 TRUE 0 306 159 101 89 67 1 1 1 15 NA NA 26.50212 FALSE 1 686 165 116 105 95 2 5 1 15 6 2 34.89440 TRUE 1 2685 176 105 106 78 1 2 0 21 2 1 25.18079 FALSE 0 3122 164 99 78 65 1 3 1 19 3 1 24.16716 FALSE 0 592 164 102 86 67 1 1 1 13 3 1 24.91077 FALSE 0 95 166 95 82 58 1 3 1 18 2 1 21.04805 FALSE 0 2446 167 90 92 66 1 5 0 10 8 2 23.66524 FALSE 1 3079 164 122 88 98 1 1 1 16 0 1 36.43664 TRUE 0 1284 160 107 102 74 1 3 1 15 5 2 28.90625 FALSE 1 996 162 114 95 88 1 1 1 18 NA NA 33.53147 TRUE 1 2072 173 106 91 72 1 1 1 18 1 1 24.05693 FALSE 0 1357 176 134 139 133 1 1 1 15 NA NA 42.93647 TRUE 1 526 176 100 95 93 1 1 0 16 NA NA 30.02324 TRUE 1 26 184 115 108 107 1 1 0 13 2 1 31.60444 TRUE 0 1759 170 111 113 106 1 1 0 15 1 1 36.67820 TRUE 0 Based on this sample, what would be your best guess as to what the true mean height of the entire population is? Since the sample is a simple random sample, you would probably go with the average: 168.75. But how certain are you that your estimate is a good? What’s to say that it’s not super far from the true population mean height? One way to answer this question is by thinking about the distribution of the average of 20 samples. If we can get an idea of what the distribution of this is compared to the true population mean height (which we know in this case, since the SHOW cohort is the entire population), then we can maybe say something about how likely we are to be “close” to the population mean. To get a better idea of what the distribution of the sample average is, we can create many, many samples of size 20 from the population, calculate the average height for each of them, and then create a histogram. Since we have the entire population available, we can also calculate the true population mean height, and then see how the distribution of the sample average compares. So, let us do just that. First of all, the true population mean height is 169.4243354, which is simply the average of ALL subjects in the population. Furthermore, we can consider the distribution of the individual heights: Figure 8.1: Population distribution of height Now, the first sample we got gave us an average of 168.75. We resample from the population, and this time end up with this sample: id height hip waste weight race marital gender edu depression_score depression_severity bmi obesity depression_severity_binary 576 168 127 124 112 4 3 0 15 11 3 39.68254 TRUE 1 2091 183 109 108 104 1 2 0 20 7 2 31.05497 TRUE 1 2395 177 101 95 86 1 1 0 13 5 2 27.45060 FALSE 1 2807 168 112 101 88 1 1 1 12 2 1 31.17914 TRUE 0 2022 153 80 70 43 1 6 1 19 8 2 18.36900 FALSE 1 1096 166 104 101 82 1 5 1 18 NA NA 29.75758 FALSE 1 3082 179 106 103 95 1 1 0 10 1 1 29.64951 FALSE 0 3359 164 115 105 93 1 1 1 18 NA NA 34.57763 TRUE 1 438 170 113 101 84 1 1 1 15 3 1 29.06574 FALSE 0 919 170 141 138 127 1 1 1 13 4 1 43.94464 TRUE 0 321 180 105 108 95 1 1 0 15 3 1 29.32099 FALSE 0 2076 183 146 145 152 1 1 0 18 4 1 45.38804 TRUE 0 2848 173 108 101 99 1 1 0 15 7 2 33.07829 TRUE 1 251 161 93 71 49 1 3 1 18 3 1 18.90359 FALSE 0 2953 165 100 87 72 1 1 0 17 2 1 26.44628 FALSE 0 3264 175 99 99 77 1 1 1 18 NA NA 25.14286 FALSE 1 1374 163 146 147 134 1 1 1 18 2 1 50.43472 TRUE 0 562 171 120 119 116 1 1 0 16 3 1 39.67033 TRUE 0 543 167 100 97 76 1 1 0 12 0 1 27.25089 FALSE 0 1860 177 96 87 72 1 3 0 13 6 2 22.98190 FALSE 1 As you can see, in this sample we have different subjects (i.e. different id’s), as we would expect when sampling only 20 subjects out of a total of 2934. From this new sample, we get a sample average of 170.65. As you can see, this is indeed different than the average height of the first sample. Now, we do this over and over and over again, a total of 10^{4} times. So, in the end, we have 10^{4} samples, and for each sample, we calculate an average. All of these averages can be used to create a histogram, which gives us a great approximation of the distribution of the sample average (with n = 20): Figure 8.2: Distribution of average heights. A few things to note here: Look how nicely the distribution is centered around the true population mean! This mean that using the sample average as an estimator of the true population mean might not be an entirely bad idea: in general, we are more likely to get an average that is “close” to the truth! The shape of that distribution looks an awful lot like a normal distribution, don’t you think? Coincidence? Maybe. Maybe not… This histogram is a lot narrower than that of the actual heights. To really see that, the figure below shows both distributions overlayed one another. This tells us that to get a good idea of the true mean population height, it’s a much better idea to create a sample of 20 subjects and use their average as your best guess than to simply sample a single individual, and use their height. Probably not surprising. But if you think of the height of a single individual as “an average of a sample of size 1”, and the true value as “an anverage of a sample of size \\(\\infty\\)” (here, \\(\\infty\\) equals the total population), then you might realize a pattern: the small sample size (sample size of 1) is worse than the medium sample size (20), which is worse than the ideal sample size (\\(\\infty\\)). It seems that your guess gets better as you increase the sample size… Coincidence? Maybe. Maybe not… Estimating Mean Depression Score Of the three bullet points above, the one that to me is the most surprising is the second one. Points 1 and 3 seem pretty intuitive: the former says that the average is a good substitute for the mean, the third that bigger sample size is better. Not exactly mind blowing. The second one, however, is more intriguing, although in the previous case, maybe not so much. After all, the distribution of the population (i.e. the distribution of all heights, shown in 8.1) looks a whole lot like a normal distribution in the first place. Let’s take a look at what happens if we consider something that is nothing like a normal distribution. Let’s say we would like to estimate the mean depression score in the population. The procedure is the same as before. Take a sample, calculate the average, repeat a bunch of times to get a good approximation of the distribution. Here, we take samples of 50. The first sample came out to consist of the following subjects: id depression_score height hip waste weight race marital gender edu depression_severity bmi obesity depression_severity_binary 1973 0 184 96 86 79 1 1 0 13 1 23.33412 FALSE 0 2730 0 164 103 86 69 1 1 1 15 1 25.65437 FALSE 0 2481 1 186 104 103 95 1 1 0 13 1 27.45982 FALSE 0 1766 0 177 105 99 88 1 1 0 15 1 28.08899 FALSE 0 1291 0 166 108 81 66 1 1 1 13 1 23.95123 FALSE 0 3058 13 173 104 112 94 1 5 0 10 3 31.40766 TRUE 1 1786 6 168 103 81 70 1 2 1 19 2 24.80159 FALSE 1 646 0 158 124 123 104 2 5 1 15 1 41.65999 TRUE 0 2562 3 171 84 70 51 1 1 1 15 1 17.44126 FALSE 0 2144 1 179 106 93 77 1 1 1 18 1 24.03171 FALSE 0 2292 3 157 93 69 53 1 1 1 18 1 21.50189 FALSE 0 2337 19 163 101 92 64 1 1 1 13 4 24.08822 FALSE 1 2885 7 162 137 126 121 1 1 1 15 2 46.10578 TRUE 1 474 1 178 94 84 71 3 1 0 20 1 22.40879 FALSE 0 331 1 168 96 82 62 2 5 1 15 1 21.96712 FALSE 0 2529 3 175 113 112 106 4 1 0 12 1 34.61224 TRUE 0 2627 5 179 127 127 127 1 1 0 13 2 39.63672 TRUE 1 2623 4 162 125 107 101 1 1 1 17 1 38.48499 TRUE 0 3087 0 174 109 110 87 1 1 0 12 1 28.73563 FALSE 0 2756 0 171 101 93 83 4 5 0 9 1 28.38480 FALSE 0 1056 3 169 103 75 67 1 3 1 13 1 23.45856 FALSE 0 2916 0 183 111 107 102 1 1 0 13 1 30.45776 TRUE 0 2857 1 168 102 80 74 1 1 1 16 1 26.21882 FALSE 0 2551 3 166 134 128 109 1 1 1 15 1 39.55581 TRUE 0 2254 13 180 136 150 165 1 3 0 16 3 50.92593 TRUE 1 177 1 154 106 82 72 3 6 1 17 1 30.35925 TRUE 0 2393 3 169 112 106 84 1 3 1 15 1 29.41073 FALSE 0 2909 2 167 95 79 62 1 1 1 13 1 22.23099 FALSE 0 2404 0 174 103 94 82 1 5 1 18 1 27.08416 FALSE 0 1255 1 175 111 106 84 4 3 1 19 1 27.42857 FALSE 0 2711 0 183 109 122 101 1 1 0 15 1 30.15916 TRUE 0 2042 2 161 101 80 60 1 1 1 17 1 23.14726 FALSE 0 1222 0 166 113 95 86 2 1 1 15 1 31.20917 TRUE 0 604 5 173 109 114 99 1 3 0 13 2 33.07829 TRUE 1 2715 0 164 104 86 69 1 1 1 18 1 25.65437 FALSE 0 2817 0 176 107 95 94 1 5 0 18 1 30.34607 TRUE 0 3000 1 178 93 90 74 1 1 0 18 1 23.35564 FALSE 0 3137 5 171 94 85 66 1 5 0 17 2 22.57105 FALSE 1 2218 5 168 107 99 80 1 1 1 15 2 28.34467 FALSE 1 1374 2 163 146 147 134 1 1 1 18 1 50.43472 TRUE 0 65 2 167 147 136 133 1 5 1 15 1 47.68905 TRUE 0 2281 1 166 103 100 84 1 1 0 16 1 30.48338 TRUE 0 2276 0 179 99 86 78 1 5 0 19 1 24.34381 FALSE 0 1478 8 177 128 114 121 4 5 1 16 2 38.62236 TRUE 1 1418 5 169 133 124 108 1 1 1 13 2 37.81380 TRUE 1 1412 8 167 103 103 95 1 5 0 13 2 34.06361 TRUE 1 1479 2 172 106 115 95 1 1 0 16 1 32.11195 TRUE 0 435 1 176 101 93 80 1 5 0 15 1 25.82645 FALSE 0 391 6 152 117 89 78 1 1 1 16 2 33.76039 TRUE 1 2439 8 167 140 126 113 1 1 1 18 2 40.51777 TRUE 1 The average depression score in this sample is 3.1. Rinse and repeat 10^{4} times. Before we take a look at the distribution of all the averages, let’s consider the distribution of the depression scores in the entire population. This is nothing like a normal distribution at all! By nature, this distribution is discrete (each observation is a score from 0 to 25), and it is not symmetrical around the mean. But take a look at what the distribution of the averages looks like: Pretty symmetrical. Centered around the true mean. Looks pretty bell-shaped to me. In other words, saying that this distribution is (at least approximately) normal does not seem like a stretch to me! Estimating Proportion of Men Next, let’s consider what to do if we were instead interested in the proportion of the population that are men From a simple random sample of size 20, I would argue that the best guess for the true proportion of women in the population is the sample proportion: the number of women out of the total number of individuals in the sample. Seems intuitively sound. Let’s got through the same motion that we did with the means above: sample a bunch of times from the population, each time calculate the sample proportion, then consider the histogram. First, the true distribution of the gender variable in the data. Here, \\(0\\) is stand-in for women, \\(1\\) stand-in for men. We see that the true proportion of the population that are men is 0.56. Next, let’s take a look at the distribution of sample proportions. Again, it looks pretty normal! How can that be?! The truth is, as we will see later on, a proportion is really not that different from an average. Since the gender variable is \\(1\\) for all men, and \\(0\\) for all women, then the proportion of men is really calculated as \\(\\frac{1}{n}\\sum_{i=1}^n g_i\\), where \\(g_i\\) is \\(1\\) if the \\(i\\)’th subject is male, and \\(0\\) otherwise. So, the proportion is really an average, and therefore it might not be that big of a surprise that the distribution of the sample proportions is approximately normal. #### Estimating Relative Risk {-} So far, we’ve seen three examples, but they’ve really all dealt with one estimator: namely the average. (As mentioned, even the proportion can be considered an average.) Let’s turn to something that does NOT turn out to be normally distributed. Say we are interested in the relative risk of being severely depressed between men and women. It seems reasonable that a good estimate of the relative risk in the population is simply the relative risk in the sample we get. Let’s take a look. We create simple random samples of size 50. The first sample consists of the following individuals: id depression_severity_binary gender height hip waste weight race marital edu depression_score depression_severity bmi obesity 2850 0 1 162 94 81 68 3 1 18 2 1 25.91068 FALSE 154 0 1 165 85 75 50 1 1 18 3 1 18.36547 FALSE 2470 0 0 167 91 83 65 1 5 16 2 1 23.30668 FALSE 615 1 1 NA NA NA NA 2 1 16 NA NA NA NA 1026 1 0 NA NA NA NA 1 1 16 NA NA NA NA 459 1 0 177 100 92 78 1 1 20 NA NA 24.89706 FALSE 549 0 1 168 139 116 109 1 1 15 3 1 38.61961 TRUE 1333 1 1 178 128 115 114 1 1 17 NA NA 35.98031 TRUE 2405 0 1 163 86 68 46 1 5 13 4 1 17.31341 FALSE 1087 1 0 175 106 109 102 1 1 18 6 2 33.30612 TRUE 483 1 1 154 96 78 54 1 1 18 NA NA 22.76944 FALSE 2533 0 0 169 111 112 103 1 1 13 2 1 36.06316 TRUE 2 1 0 162 49 71 53 3 5 13 NA NA 20.19509 FALSE 296 1 0 184 100 87 83 2 5 15 NA NA 24.51560 FALSE 437 0 1 172 99 93 74 1 1 19 0 1 25.01352 FALSE 1744 0 0 170 96 90 73 2 1 15 0 1 25.25952 FALSE 3352 1 0 170 90 88 64 NA 1 16 NA NA 22.14533 FALSE 144 1 1 154 112 81 68 3 1 19 12 3 28.67263 FALSE 875 1 0 168 108 120 104 1 1 16 NA NA 36.84807 TRUE 31 1 0 186 106 105 95 1 1 16 NA NA 27.45982 FALSE 877 0 0 176 96 94 75 1 5 15 3 1 24.21229 FALSE 1454 1 1 162 108 102 73 1 1 18 NA NA 27.81588 FALSE 386 0 0 160 113 119 96 1 1 13 0 1 37.50000 TRUE 1434 1 1 151 102 102 69 1 3 16 NA NA 30.26183 TRUE 2605 1 1 157 121 117 90 1 1 15 5 2 36.51264 TRUE 2752 0 1 166 103 80 103 1 3 15 1 1 37.37843 TRUE 1619 0 0 194 99 88 84 1 1 21 1 1 22.31906 FALSE 423 1 0 166 96 101 77 1 5 13 10 3 27.94310 FALSE 2411 0 0 180 120 125 115 1 1 19 1 1 35.49383 TRUE 968 0 1 171 81 70 50 1 1 15 1 1 17.09928 FALSE 2239 0 0 178 100 90 80 1 1 19 0 1 25.24934 FALSE 918 0 1 170 90 69 54 1 3 15 0 1 18.68512 FALSE 2105 0 0 161 88 82 60 4 1 21 1 1 23.14726 FALSE 1680 0 0 179 113 105 100 1 1 16 4 1 31.21001 TRUE 45 0 1 170 95 77 60 1 6 18 1 1 20.76125 FALSE 137 0 1 159 100 94 70 1 2 15 3 1 27.68878 FALSE 100 1 0 175 106 103 95 1 1 15 8 2 31.02041 TRUE 401 1 1 169 115 108 93 1 1 19 NA NA 32.56189 TRUE 787 1 1 163 134 117 95 1 3 17 16 4 35.75596 TRUE 2036 0 1 163 109 110 80 1 1 18 0 1 30.11028 TRUE 27 0 1 170 98 88 63 1 1 15 0 1 21.79931 FALSE 2065 0 1 172 97 75 63 1 5 13 0 1 21.29529 FALSE 571 0 0 169 115 115 107 2 6 17 4 1 37.46367 TRUE 930 1 0 170 90 80 56 4 1 21 NA NA 19.37716 FALSE 1437 0 1 160 36 28 54 1 1 16 1 1 21.09375 FALSE 451 1 1 160 126 114 103 1 5 15 NA NA 40.23438 TRUE 1409 0 0 181 99 96 81 1 5 14 4 1 24.72452 FALSE 2735 0 0 181 102 98 102 1 3 16 3 1 31.13458 TRUE 2644 1 1 NA NA NA NA 1 1 15 NA NA NA NA 1332 1 1 159 120 107 96 1 1 13 NA NA 37.97318 TRUE To find the relative risk, we create the 2 by 2 contingency table for gender and depression_severity_binary: depression_severity_binary Female Male Total 0 13 14 27 1 11 12 23 Total 24 26 50 The relative risk is then calculated as $ \\[\\begin{align} \\frac{\\text{proportion of males with severe depression}}{\\text{proportion of women with severe depression}} &amp;= \\frac{17/29}{11/21} \\\\ &amp; \\approx 1.01. \\end{align}\\] $ As before, we repeat this many, many times, and plot the results as a histogram, which gives us an approximate distribution of the relative risk in a sample of 50 subjects. The first thing we probably notice is that this is not a normal distribution. It is not symmetrical, and therefore also not bell-shaped. However, it is very nicely distributed around the true population relative risk. So, for the first time we end up with something that is not normally distributed. This is not in and of itself a huge problem, but it does make life a bit harder later on. In this particular case, however, there is a very simple fix: instead of considering the relative risk, consider \\(\\log(RR)\\), the log transformed relative risk: Looks pretty normal, huh? We will (ab)use this fact later on. Estimating Odds Ratios Here we repeat the previous section, but estimating the odds ratio instead of the relative risk. Same comments apply. Definitely not normal. But if we log transform… 8.3 Deriving Distributions in Practice In the previous section, we considered quite a few examples of estimators, and saw how we can get a very good idea of exactly what the distribution of an estimator is when we have the entire population at our disposal. This is basically never the case. Even if we had a way of getting in touch with the entire popoulation, and create thousands and thousands of simple random samples, it is very unlikely we would have the time and funds to do so. So, in practice, we have to do something else to find the distribution of the estimator we’re interested in. Here, we will discuss how to do so using what I think is the coolest result we will encounter in this class, namely the Central Limit Theorem. 8.3.1 The Central Limit Theorem Let’s jump right to it, and state the Central Limit Theorem: (#thm:The Central Limit Theorem) Let \\(X_1, X_2, ..., X_n\\) be a simple random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\) (i.e. \\(E(X_i) = \\mu\\) and \\(\\text{Var}(X_i) = \\sigma^2\\) for all \\(i\\)). Then, as long as \\(n\\) is large enough, the average \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is approximately \\(N(\\mu, \\sigma^2 / n)\\). So what’s so special about this? It’s a rather simple setup: if you have a simple random sample of size “big enough”, then the average is going to be normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). But think about this for a second: there are no assumptions on where you start, and yet we get that we end up with something that is normally distributed! Let’s revisit the examples from above. Estimating Mean Height We saw previously that if we create 10^{4} samples from the SHOW population, calculate the average height of each sample, and then create a histogram of all these averages, it would result in something like figure 8.2. Just by looking at this, we observed it looked a lot like a normal distribution. Now, with the CLT at hand, we can actually find the exact normal distribution that it follows. Since we know the true mean and variance (the mean and variance of the entire population), we can find the specific normal distribution that the CLT tells us this distribution should be much like. From the SHOW population, we find that the mean height is \\(\\mu = 169.4243354\\), and the variance is \\(\\sigma^2 = 98.7128205\\). So, since the sample size here was \\(n = 20\\), the CLT tells us that \\(\\bar{X} \\sim N(169.4243354, 98.7128205/20)\\). How does this fit the histogram? The black line below is that exact normal distribution. Fits pretty well, if you ask me. Estimating Mean Depression Score We argued above that the fact that the average heights follow a normal distribution isn’t all that surprising. But we also saw that the mean depression score actually looks like something that is normally distributed. Well, again, we can use the CLT to calculate the normal distribution it should follow. The mean depression score in the entire population is 3.49, and the variance of the depression scores in the population is 16.55. So, the average depression score of our samples (that have sample size 50), should follow a normal distribution with mean 3.49 and variance 0.331. Again, pretty spot on! Estimating Proportion of Men For completion, we consider the last example from above to which the CLT applies. When estimating the proportion of men in the population, the sample proportion also follows a normal distribution, and again we can calculate the mean and variance of that distribution. Even more examples for the curious To explore more, you can spend a few minutes here. 8.3.2 Distributions of Common Estimators "],
["about-assumptions.html", "A About Assumptions", " A About Assumptions "],
["simulating-data.html", "B Simulating Data", " B Simulating Data "]
]
